{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a static dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "static = spark.read.json(\"./../../../data/activity-data/\")\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|           x|           y|           z|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand| 3.356934E-4|-5.645752E-4|-0.018814087|\n",
      "|1424686735292|1424688581345918092|nexus4_2|   66|nexus4|   g|stand|-0.005722046| 0.029083252| 0.005569458|\n",
      "|1424686735500|1424686733498505625|nexus4_1|   99|nexus4|   g|stand|   0.0078125|-0.017654419| 0.010025024|\n",
      "|1424686735691|1424688581745026978|nexus4_2|  145|nexus4|   g|stand|-3.814697E-4|   0.0184021|-0.013656616|\n",
      "|1424686735890|1424688581945252808|nexus4_2|  185|nexus4|   g|stand|-3.814697E-4|-0.031799316| -0.00831604|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a streaming version of the same dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note below that we provide dataSchema to streaming dataframe\n",
    "# As it usually does not infer schema by default\n",
    "# It is possible to enable this but it is not recommended.\n",
    "\n",
    "streaming = spark.readStream\\\n",
    "  .schema(dataSchema)\\\n",
    "  .option(\"maxFilesPerTrigger\", 1)\\\n",
    "  .json(\"./../../../data/activity-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that streaming dataframe creation and execution is also lazy evaluation. \n",
    "# We do not get any results until we call a trigger. For this we define a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also redefine partitions given we are working in local mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also define queryname, format & output mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityQuery = activityCounts\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"activity_counts\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the currently active stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.sql.streaming.StreamingQuery at 0x7f793838c150>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a sample query on streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|        gt| count|\n",
      "+----------+------+\n",
      "|       sit|640030|\n",
      "|     stand|592006|\n",
      "|stairsdown|486880|\n",
      "|      walk|689284|\n",
      "|  stairsup|543798|\n",
      "|      null|543227|\n",
      "|      bike|561431|\n",
      "+----------+------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n",
      "+----------+-------+\n",
      "|        gt|  count|\n",
      "+----------+-------+\n",
      "|       sit| 984714|\n",
      "|     stand| 910783|\n",
      "|stairsdown| 749059|\n",
      "|      walk|1060402|\n",
      "|  stairsup| 836598|\n",
      "|      null| 835725|\n",
      "|      bike| 863710|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection and filtering on streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How does this relate to normal / static dataframes \n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "  .where(\"stairs\")\\\n",
    "  .where(\"gt is not null\")\\\n",
    "  .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "  .writeStream\\\n",
    "  .queryName(\"simple_transform\")\\\n",
    "  .format(\"memory\")\\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregations on streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Link for cube type aggregates: https://www.google.com/search?sca_esv=23c20ded85caa685&q=cube+aggregation&uds=ADvngMiIMiMH9LyyITANaU-tP7TxCmsoapubbU4OF6YqN-dZR4m4_WWghjU5yz_1J6-ynKU_M7kL14jWgtIqPtYBvL-zarvc5Enp6a1FEYBykz_OmCmfpiWy5Se2LLLBpw3ODL9hcVnl2HZYIZu2W6IA6vJ_yW2mg69oNiTPhk86Xhj2lIafZuBb-Uo6gSED5R6h8t33BVlYR3sdFM_QCDeACTgyqMzu9V9BqjkoVxty-I_tMphi-9XQ8sPJhh2XpYMRp_p63UshkFzc4gJwYoA60iYkvgW4cafb3ILDYc4w7j79XlXprMY&udm=2&prmd=ivnbz&sa=X&ved=2ahUKEwjN9J75qcWGAxWPg_0HHQReAtgQtKgLegQIDBAB&biw=1536&bih=839&dpr=2#vhid=nIWtuhkG5fLMyM&vssid=mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.cube(\"gt\", \"model\").avg()\\\n",
    "  .drop(\"avg(Arrival_time)\")\\\n",
    "  .drop(\"avg(Creation_Time)\")\\\n",
    "  .drop(\"avg(Index)\")\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins on streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historicalAgg = static.groupBy(\"gt\", \"model\").avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceModelStats = streaming.drop(\"Arrival_Time\", \"Creation_Time\", \"Index\")\\\n",
    "  .cube(\"gt\", \"model\").avg()\\\n",
    "  .join(historicalAgg, [\"gt\", \"model\"])\\\n",
    "  .writeStream.queryName(\"device_counts\").format(\"memory\")\\\n",
    "  .outputMode(\"complete\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"subscribe\", \"topic1\")\\\n",
    "  .load()\n",
    "\n",
    "# Subscribe to multiple topics\n",
    "df2 = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"subscribe\", \"topic1,topic2\")\\\n",
    "  .load()\n",
    "# Subscribe to a pattern\n",
    "df3 = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"subscribePattern\", \"topic.*\")\\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.selectExpr(\"topic\", \"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "  .start()\n",
    "\n",
    "df1.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\\\n",
    "  .writeStream\\\n",
    "  .format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\\\n",
    "  .option(\"checkpointLocation\", \"/to/HDFS-compatible/dir\")\\\n",
    "  .option(\"topic\", \"topic1\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data from sockets: nc -lk 9999\n",
    "Link: https://medium.com/@jianglancao/finally-data-cube-aggregation-can-work-directly-in-google-bigquery-50976305b9ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "socketDF = spark.readStream.format(\"socket\")\\\n",
    "  .option(\"host\", \"localhost\").option(\"port\", 9999).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff21025d4d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityCounts.writeStream.trigger(processingTime='5 seconds')\\\n",
    "  .format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x7ff260a6e950>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activityCounts.writeStream.trigger(once=True)\\\n",
    "  .format(\"console\").outputMode(\"complete\").start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Workdesk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
