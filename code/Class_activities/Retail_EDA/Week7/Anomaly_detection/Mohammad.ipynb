{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Data set availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset availability: https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "\n",
    "Full dataset: http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\n",
    "\n",
    "10% dataset:http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "\n",
    "# Uncomment these if you would like to make any graphs using Matplotlib\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib.patches as mpatches\n",
    "\n",
    "# plt.style.use('ggplot')\n",
    "# plt.rcParams['figure.figsize'] = (20.0, 8.0)\n",
    "\n",
    "# %matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initiate Spark session & load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('app').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r\"C:\\Users\\douaa\\OneDrive\\Desktop\\DSS Program\\DSS 2 Big Data\\kddcup.data.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSchema = StructType([ \\\n",
    "    StructField('duration', IntegerType(), True), \\\n",
    "    StructField('protocol_type', StringType(), True), \\\n",
    "    StructField('service', StringType(), True), \\\n",
    "    StructField('flag', StringType(), True), \\\n",
    "    StructField('src_bytes', IntegerType(), True), \\\n",
    "    StructField('dst_bytes', IntegerType(), True), \\\n",
    "    StructField('land', StringType(), True), \\\n",
    "    StructField('wrong_fragment', IntegerType(), True), \\\n",
    "    StructField('urgent', IntegerType(), True), \\\n",
    "    StructField('hot', IntegerType(), True), \\\n",
    "    StructField('num_failed_logins', IntegerType(), True), \\\n",
    "    StructField('logged_in', StringType(), True), \\\n",
    "    StructField('num_compromised', IntegerType(), True), \\\n",
    "    StructField('root_shell', IntegerType(), True), \\\n",
    "    StructField('su_attempted', IntegerType(), True), \\\n",
    "    StructField('num_root', IntegerType(), True), \\\n",
    "    StructField('num_file_creations', IntegerType(), True), \\\n",
    "    StructField('num_shells', IntegerType(), True), \\\n",
    "    StructField('num_access_files', IntegerType(), True), \\\n",
    "    StructField('num_outbound_cmds', IntegerType(), True), \\\n",
    "    StructField('is_host_login', StringType(), True), \\\n",
    "    StructField('is_guest_login', StringType(), True), \\\n",
    "    StructField('count', IntegerType(), True), \\\n",
    "    StructField('srv_count', IntegerType(), True), \\\n",
    "    StructField('serror_rate', FloatType(), True), \\\n",
    "    StructField('srv_serror_rate', FloatType(), True), \\\n",
    "    StructField('rerror_rate', FloatType(), True), \\\n",
    "    StructField('srv_rerror_rate', FloatType(), True), \\\n",
    "    StructField('same_srv_rate', FloatType(), True), \\\n",
    "    StructField('diff_srv_rate', FloatType(), True), \\\n",
    "    StructField('srv_diff_host_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_count', IntegerType(), True), \\\n",
    "    StructField('dst_host_srv_count', IntegerType(), True), \\\n",
    "    StructField('dst_host_same_srv_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_diff_srv_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_same_src_port_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_srv_diff_host_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_serror_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_srv_serror_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_rerror_rate', FloatType(), True), \\\n",
    "    StructField('dst_host_srv_rerror_rate', FloatType(), True), \\\n",
    "    StructField('type', StringType(), True) \\\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .format('csv') \\\n",
    "    .options(header='True') \\\n",
    "    .options(delimiter=',') \\\n",
    "    .load(input_file, schema=dataSchema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|duration|protocol_type|service|flag|src_bytes|dst_bytes|land|wrong_fragment|urgent|hot|num_failed_logins|logged_in|num_compromised|root_shell|su_attempted|num_root|num_file_creations|num_shells|num_access_files|num_outbound_cmds|is_host_login|is_guest_login|count|srv_count|serror_rate|srv_serror_rate|rerror_rate|srv_rerror_rate|same_srv_rate|diff_srv_rate|srv_diff_host_rate|dst_host_count|dst_host_srv_count|dst_host_same_srv_rate|dst_host_diff_srv_rate|dst_host_same_src_port_rate|dst_host_srv_diff_host_rate|dst_host_serror_rate|dst_host_srv_serror_rate|dst_host_rerror_rate|dst_host_srv_rerror_rate|   type|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "|       0|          tcp|   http|  SF|      239|      486|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    8|        8|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            19|                19|                   1.0|                   0.0|                       0.05|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "|       0|          tcp|   http|  SF|      235|     1337|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    8|        8|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            29|                29|                   1.0|                   0.0|                       0.03|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "|       0|          tcp|   http|  SF|      219|     1337|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    6|        6|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            39|                39|                   1.0|                   0.0|                       0.03|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "|       0|          tcp|   http|  SF|      217|     2032|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    6|        6|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            49|                49|                   1.0|                   0.0|                       0.02|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "|       0|          tcp|   http|  SF|      217|     2032|   0|             0|     0|  0|                0|        1|              0|         0|           0|       0|                 0|         0|               0|                0|            0|             0|    6|        6|        0.0|            0.0|        0.0|            0.0|          1.0|          0.0|               0.0|            59|                59|                   1.0|                   0.0|                       0.02|                        0.0|                 0.0|                     0.0|                 0.0|                     0.0|normal.|\n",
      "+--------+-------------+-------+----+---------+---------+----+--------------+------+---+-----------------+---------+---------------+----------+------------+--------+------------------+----------+----------------+-----------------+-------------+--------------+-----+---------+-----------+---------------+-----------+---------------+-------------+-------------+------------------+--------------+------------------+----------------------+----------------------+---------------------------+---------------------------+--------------------+------------------------+--------------------+------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using only numerical features, identify any anomalies in network connections. Remember, an anomaly is a data point, which does not fit in a 'reasonable' set of clusters for any given dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the above model, also include categorical features and determine any anomalies in network connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Finally, make a 3D graph of data points using three dimensions to visualize anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas DataFrame\n",
    "# Sometimes it is more convinient to convert Spark DF to Pandas DF for exploration\n",
    "import pandas as pd \n",
    "df_pd=df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Missing value with proportion in Pandas DataFrame#Check Missing value with proportion in Pandas DataFrame\n",
    "\n",
    "def missing_values_table(df_pd):\n",
    "    \"\"\"Input pandas dataframe and Return columns with missing value&percentage and stored as pandas dataframe\"\"\"\n",
    "    \n",
    "    mis_val = df_pd.isnull().sum() #count total of null in each columns in dataframe\n",
    "    mis_val_percent = 100 * df_pd.isnull().sum() / len(df_pd) #count percentage of null in each columns\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)  #join to left (as column) between mis_val and mis_val_percent and create it as dataframe\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'}) #rename columns in table, mis_val to Missing Values and mis_val_percent to % of Total Values\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "    mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "    '% of Total Values', ascending=False).round(1)         #sort column % of Total Values descending and round 1 after point(coma)\n",
    "    print (\"Your selected dataframe has \" + str(df_pd.shape[1]) + \" columns.\\n\"    #.shape[1] : just view total columns in dataframe  \n",
    "    \"There are \" + str(mis_val_table_ren_columns.shape[0]) +              \n",
    "    \" columns that have missing values.\") #.shape[0] : just view total rows in dataframe\n",
    "    return mis_val_table_ren_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your selected dataframe has 42 columns.\n",
      "There are 0 columns that have missing values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values, % of Total Values]\n",
       "Index: []"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_values_table(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+---------+----------------+\n",
      "|duration|src_bytes|dst_bytes|distanceToCenter|\n",
      "+--------+---------+---------+----------------+\n",
      "|       0|      300|    42747|    1.77689101E9|\n",
      "|       0|      293|    38125|    1.40865894E9|\n",
      "|       0|      284|    43129|    1.80926221E9|\n",
      "|       0|      212|    43129|    1.80936461E9|\n",
      "|       0|      284|    43129|    1.80922701E9|\n",
      "|       0|      226|    74301|     5.4323031E9|\n",
      "|       0|      188|    74810|     5.5076367E9|\n",
      "|       0|      198|    74810|     5.5076142E9|\n",
      "|       0|      256|   125015|   1.54792335E10|\n",
      "|       0|      332|    61480|    3.70673382E9|\n",
      "|       0|      226|    61480|    3.70687744E9|\n",
      "|       0|      236|    39873|    1.54299661E9|\n",
      "|       0|      231|    36340|     1.2779831E9|\n",
      "|       0|      309|    81172|     6.4921482E9|\n",
      "|       0|      330|    80476|     6.3804411E9|\n",
      "|       0|      212|    39751|     1.5334528E9|\n",
      "|       0|      203|    39751|    1.53347533E9|\n",
      "|       0|      228|    44578|    1.93466867E9|\n",
      "|       0|      230|    33924|    1.11114931E9|\n",
      "|       0|      214|    33924|    1.11113203E9|\n",
      "+--------+---------+---------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "\n",
    "if 'features' in df.columns:\n",
    "    df = df.drop('features')\n",
    "    \n",
    "\n",
    "# Define categorical columns\n",
    "categoricalCols = ['protocol_type', 'service', 'flag', 'land', 'logged_in', 'is_host_login', 'is_guest_login']\n",
    "\n",
    "# Filter out categorical columns with only one distinct value\n",
    "valid_categoricalCols = [col for col in categoricalCols if df.select(col).distinct().count() > 1]\n",
    "\n",
    "# Create indexers and encoders\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in valid_categoricalCols]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\") for col in valid_categoricalCols]\n",
    "\n",
    "# Create the stages list for the Pipeline\n",
    "stages = indexers + encoders\n",
    "\n",
    "# Define numerical columns\n",
    "numericalCols = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', \n",
    "                 'num_compromised', 'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', \n",
    "                 'num_access_files', 'num_outbound_cmds', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', \n",
    "                 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', \n",
    "                 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', \n",
    "                 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', \n",
    "                 'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
    "\n",
    "# Assemble features\n",
    "assemblerInputs = [col + \"_vec\" for col in valid_categoricalCols] + numericalCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages.append(assembler)\n",
    "\n",
    "# Apply the stages to the DataFrame\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(df)\n",
    "df = pipelineModel.transform(df)\n",
    "\n",
    "# Apply K-means clustering\n",
    "kmeans = KMeans(k=10, seed=1, featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "model = kmeans.fit(df)\n",
    "transformed = model.transform(df)\n",
    "\n",
    "# Calculate distances to cluster centers to identify outliers\n",
    "def get_dist_to_center(point, centers, cluster):\n",
    "    center = centers[cluster]\n",
    "    return float(Vectors.squared_distance(Vectors.dense(point), Vectors.dense(center)))\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "distance_udf = udf(lambda point, cluster: get_dist_to_center(point, centers, cluster), FloatType())\n",
    "\n",
    "transformed = transformed.withColumn(\"distanceToCenter\", distance_udf(col(\"features\"), col(\"prediction\")))\n",
    "\n",
    "# Define a threshold for identifying outliers\n",
    "threshold = transformed.agg({\"distanceToCenter\": \"mean\"}).collect()[0][0] * 3\n",
    "outliers = transformed.filter(col(\"distanceToCenter\") > threshold)\n",
    "\n",
    "# Show anomalies\n",
    "outliers.select(\"duration\", \"src_bytes\", \"dst_bytes\", \"distanceToCenter\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error collecting chunk starting at 0: An error occurred while calling o1966.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 145.0 failed 1 times, most recent failure: Lost task 0.0 in stage 145.0 (TID 3328, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 148, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n",
      "    obj = stream.read(length)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n",
      "\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3263)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3260)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3260)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\n",
      "\tat java.lang.reflect.Method.invoke(Unknown Source)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Unknown Source)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 377, in main\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 372, in process\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 148, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 170, in _read_with_length\n",
      "    obj = stream.read(length)\n",
      "  File \"c:\\Users\\douaa\\anaconda3\\ANACONDA\\envs\\PySpark_2_4_5\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "socket.timeout: timed out\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:255)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\n",
      "\t... 1 more\n",
      "\n",
      "Not enough data to plot.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define the sample size and chunk size\n",
    "sample_fraction = 0.001  # 0.1% sample\n",
    "chunk_size = 500  # Adjust as needed\n",
    "max_records = 10000  # Maximum number of records to collect\n",
    "\n",
    "# Sample a smaller fraction of the data\n",
    "outliers_sample = outliers.sample(False, sample_fraction, seed=42)\n",
    "\n",
    "# Select only the needed columns\n",
    "selected_columns = [\"duration\", \"src_bytes\", \"dst_bytes\", \"distanceToCenter\"]\n",
    "outliers_pd_list = []\n",
    "\n",
    "# Collect data in chunks to avoid memory issues\n",
    "collected_records = 0\n",
    "for start in range(0, outliers_sample.count(), chunk_size):\n",
    "    try:\n",
    "        chunk = outliers_sample.select(selected_columns).limit(chunk_size).collect()\n",
    "        outliers_pd_list.extend(chunk)\n",
    "        collected_records += len(chunk)\n",
    "        if collected_records >= max_records:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"Error collecting chunk starting at {start}: {e}\")\n",
    "        break\n",
    "\n",
    "# Convert collected data to Pandas DataFrame\n",
    "outliers_pd = pd.DataFrame(outliers_pd_list, columns=selected_columns)\n",
    "\n",
    "# Plotting in 3D if there are enough records\n",
    "if not outliers_pd.empty:\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    sc = ax.scatter(outliers_pd['duration'], outliers_pd['src_bytes'], outliers_pd['dst_bytes'], c=outliers_pd['distanceToCenter'], cmap=\"seismic\")\n",
    "    plt.colorbar(sc)\n",
    "    ax.set_title('Network Connection Anomalies')\n",
    "    ax.set_xlabel('Duration')\n",
    "    ax.set_ylabel('Source Bytes')\n",
    "    ax.set_zlabel('Destination Bytes')\n",
    "\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough data to plot.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
