{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/03 18:25:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '16g'\n",
    "os.environ['SPARK_EXECUTOR_MEMORY'] = '16g'\n",
    "\n",
    "# By setting SPARK_DRIVER_MEMORY and SPARK_EXECUTOR_MEMORY to '16g', you're allocating 16GB of memory for both the Spark driver and the executors.\n",
    "# this is needed to run StringIndexer in later steps\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"ratings_Beauty.csv\")\\\n",
    "  .coalesce(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserId: string (nullable = true)\n",
      " |-- ProductId: string (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      " |-- Timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# checking the columns and the data types \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns UserId and ProductId are of type string but for the ALS model they should be of type integer or double. Therefore, we need to encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:32:27 WARN DAGScheduler: Broadcasting large task binary with size 36.5 MiB\n",
      "24/06/03 18:32:40 WARN DAGScheduler: Broadcasting large task binary with size 36.5 MiB\n",
      "24/06/03 18:32:44 WARN DAGScheduler: Broadcasting large task binary with size 50.8 MiB\n",
      "[Stage 26:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+---------+\n",
      "|Rating| Timestamp|  UserId|ProductId|\n",
      "+------+----------+--------+---------+\n",
      "|   5.0|1369699200| 70392.0| 145790.0|\n",
      "|   3.0|1355443200|265306.0| 103581.0|\n",
      "|   5.0|1404691200|552933.0| 103581.0|\n",
      "|   4.0|1382572800|536779.0| 145791.0|\n",
      "|   1.0|1274227200| 14679.0| 145792.0|\n",
      "|   5.0|1404518400|    86.0| 145793.0|\n",
      "|   5.0|1371945600|   483.0| 145794.0|\n",
      "|   5.0|1373068800|  2928.0| 145795.0|\n",
      "|   5.0|1401840000|994647.0| 145796.0|\n",
      "|   4.0|1389052800|242707.0|  81247.0|\n",
      "|   5.0|1372032000|   483.0|  81247.0|\n",
      "|   4.0|1378252800|300178.0|  81247.0|\n",
      "|   5.0|1372118400| 35883.0| 145797.0|\n",
      "|   5.0|1371686400|  2928.0| 145798.0|\n",
      "|   5.0|1372118400| 35883.0| 103582.0|\n",
      "|   5.0|1373414400|  5107.0| 103582.0|\n",
      "|   5.0|1372896000|  2928.0| 103583.0|\n",
      "|   5.0|1372896000|139615.0| 103583.0|\n",
      "|   5.0|1373068800|  2928.0| 103584.0|\n",
      "|   5.0|1372291200|   483.0| 103584.0|\n",
      "+------+----------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Create StringIndexers for UserId and ProductId\n",
    "userIndexer = StringIndexer(inputCol=\"UserId\", outputCol=\"UserIdInt\", handleInvalid=\"skip\")\n",
    "productIndexer = StringIndexer(inputCol=\"ProductId\", outputCol=\"ProductIdInt\", handleInvalid=\"skip\")\n",
    "\n",
    "# Fit and transform UserId column\n",
    "df_indexed_user = userIndexer.fit(df).transform(df)\n",
    "\n",
    "# Fit and transform ProductId column\n",
    "df_indexed_product = productIndexer.fit(df_indexed_user).transform(df_indexed_user)\n",
    "\n",
    "# Drop the original string columns\n",
    "final_df = df_indexed_product.drop(\"UserId\", \"ProductId\")\n",
    "\n",
    "# Rename the encoded columns\n",
    "final_df = final_df.withColumnRenamed(\"UserIdInt\", \"UserId\").withColumnRenamed(\"ProductIdInt\", \"ProductId\")\n",
    "\n",
    "# Show the final DataFrame\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:33:22 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "[Stage 30:=====>                                                   (1 + 9) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "|summary|            Rating|           Timestamp|            UserId|         ProductId|\n",
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "|  count|           2023070|             2023070|           2023070|           2023070|\n",
      "|   mean| 4.149035871225415|1.3603887365637374E9|396279.36594087206|31677.089125438073|\n",
      "| stddev|1.3115045737121593| 4.611860421680957E7| 375833.6848674939| 50072.68503668203|\n",
      "|    min|               1.0|           908755200|               0.0|               0.0|\n",
      "|    max|               5.0|          1406073600|         1210270.0|          249273.0|\n",
      "+-------+------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "final_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternating Least Squares model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = final_df.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, \\\n",
    "          regParam=0.01, \\\n",
    "          userCol='UserId', \\\n",
    "          itemCol='ProductId', \n",
    "          ratingCol='Rating', \\\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:34:12 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:34:16 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:34:29 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:34:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:34:46 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:34:53 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:01 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:06 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/06/03 18:35:09 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:15 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n",
      "24/06/03 18:35:16 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:22 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:29 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:38 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:44 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:54 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:35:59 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:36:08 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:36:15 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:36:22 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:37:21 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:37:30 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:37:33 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:37:49 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:38:05 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "[Stage 116:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+---------+----------+\n",
      "|Rating| Timestamp|  UserId|ProductId|prediction|\n",
      "+------+----------+--------+---------+----------+\n",
      "|   1.0|1135468800|268271.0|   6552.0|       NaN|\n",
      "|   1.0|1192233600|383781.0|  11941.0|       NaN|\n",
      "|   1.0|1182211200|  1628.0|  42381.0|-1.1854051|\n",
      "|   1.0|1017273600|817867.0|  57774.0|       NaN|\n",
      "|   1.0|1214179200|     7.0|  72777.0| 2.9140964|\n",
      "+------+----------+--------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName = 'rmse',\\\n",
    "                                labelCol = 'Rating', \\\n",
    "                                predictionCol = 'prediction'\\\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:39:23 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:39:32 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:39:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:39:54 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:40:12 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "24/06/03 18:40:20 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "[Stage 167:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to use the regression evaluator and then print the results we get NaN because not all the predictions are numerical. To avoid NaNs in predictions, we can set cold start strategy to drop NaN and it will not include any unseen users or items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=5, \\\n",
    "          regParam=0.01, \\\n",
    "          userCol='UserId', \\\n",
    "          itemCol='ProductId', \n",
    "          ratingCol='Rating', \\\n",
    "          coldStartStrategy=\"drop\"\\\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:42:32 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:42:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:42:52 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:42:57 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:11 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:18 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:31 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:49 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:43:54 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:01 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:09 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:16 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:28 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:34 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:39 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:51 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:44:58 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model = als.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:45:07 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:45:15 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:45:20 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:45:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:45:48 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "[Stage 251:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-------+---------+-----------+\n",
      "|Rating| Timestamp| UserId|ProductId| prediction|\n",
      "+------+----------+-------+---------+-----------+\n",
      "|   5.0|1383696000| 6832.0|     28.0| -1.6007009|\n",
      "|   5.0|1393200000|15164.0|     28.0| -0.8685224|\n",
      "|   5.0|1387756800|37008.0|     28.0|0.075431585|\n",
      "|   5.0|1357516800|37743.0|     28.0|-0.46197078|\n",
      "|   5.0|1370390400|41810.0|     28.0| -1.6041979|\n",
      "+------+----------+-------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName = 'rmse',\\\n",
    "                                labelCol = 'Rating', \\\n",
    "                                predictionCol = 'prediction'\\\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:47:52 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:48:00 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:48:04 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:48:15 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:48:31 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "24/06/03 18:48:37 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "[Stage 302:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 6.436409561114752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rmse = evaluator.evaluate(predictions)\n",
    "print('RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error seems to be relatively high. Generally, lower RMSE values (closer to 0) are better, as they indicate that the model's predictions are more accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the model on sample users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:51:30 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:51:33 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:51:37 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "[Stage 305:===========>                                             (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "| UserId|ProductId|\n",
      "+-------+---------+\n",
      "|15164.0|     28.0|\n",
      "|15164.0|   3510.0|\n",
      "+-------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "this_user = test.filter(test['UserId'] == 15164.0).select('UserId', 'ProductId')\n",
    "this_user.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/03 18:51:58 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:52:04 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:52:08 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:52:30 WARN DAGScheduler: Broadcasting large task binary with size 50.9 MiB\n",
      "24/06/03 18:52:38 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "24/06/03 18:52:40 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "24/06/03 18:52:42 WARN DAGScheduler: Broadcasting large task binary with size 51.0 MiB\n",
      "[Stage 391:>                                                        (0 + 5) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+\n",
      "| UserId|ProductId|prediction|\n",
      "+-------+---------+----------+\n",
      "|15164.0|     28.0|-0.8685224|\n",
      "|15164.0|   3510.0| 3.9482117|\n",
      "+-------+---------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recommendation_this_user = model.transform(this_user)\n",
    "recommendation_this_user.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
