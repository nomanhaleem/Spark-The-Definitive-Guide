{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(r\"C:\\Users\\Britt\\Documents\\RUG\\Year 1\\Big Data Analystics\\heart_statlog_cleveland_hungary_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- chest pain type: integer (nullable = true)\n",
      " |-- resting bp s: integer (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- fasting blood sugar: integer (nullable = true)\n",
      " |-- resting ecg: integer (nullable = true)\n",
      " |-- max heart rate: integer (nullable = true)\n",
      " |-- exercise angina: integer (nullable = true)\n",
      " |-- oldpeak: double (nullable = true)\n",
      " |-- ST slope: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "|age|sex|chest pain type|resting bp s|cholesterol|fasting blood sugar|resting ecg|max heart rate|exercise angina|oldpeak|ST slope|target|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "| 40|  1|              2|         140|        289|                  0|          0|           172|              0|    0.0|       1|     0|\n",
      "| 49|  0|              3|         160|        180|                  0|          0|           156|              0|    1.0|       2|     1|\n",
      "| 37|  1|              2|         130|        283|                  0|          1|            98|              0|    0.0|       1|     0|\n",
      "| 48|  0|              4|         138|        214|                  0|          0|           108|              1|    1.5|       2|     1|\n",
      "| 54|  1|              3|         150|        195|                  0|          0|           122|              0|    0.0|       1|     0|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividing the data into training and testing sets\n",
    "train_df, test_df = df.randomSplit([0.6, 0.4], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[\"age\", \"sex\", \"chest pain type\", \"resting bp s\", \"cholesterol\", \"fasting blood sugar\", \"resting ecg\", \"max heart rate\", \"exercise angina\", \"oldpeak\", \"ST slope\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a logistic regression model\n",
    "lr = LogisticRegression(labelCol=\"target\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a Paramgrid\n",
    "Paramgrid_lr = (ParamGridBuilder()\n",
    "                .addGrid(lr.regParam, [0.1, 0.01])\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "                .build())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using cross validation to get the best results\n",
    "cv_lr = CrossValidator(estimator=lr, estimatorParamMaps=Paramgrid_lr, evaluator=BinaryClassificationEvaluator(labelCol=\"target\"), numFolds=5)\n",
    "cv_model_lr = cv_lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions\n",
    "lr_predictions = cv_model_lr.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating the model\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"target\")\n",
    "lr_accuracy = evaluator.evaluate(lr_predictions)\n",
    "print(lr_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
