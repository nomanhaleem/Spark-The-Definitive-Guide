{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "    # feature module\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Start a Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('nlp').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|            sentence|\n",
      "+---+--------------------+\n",
      "|  0|Hi I heard about ...|\n",
      "|  1|I wish java could...|\n",
      "|  2|Logistic,regressi...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A simple dataframe containing text data\n",
    "\n",
    "sen_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish java could use case classes'),\n",
    "    (2, 'Logistic,regression,model,are,neat')\n",
    "], ['id', 'sentence'])\n",
    "\n",
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Tokenizer and Regex tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer: A tokenizer converts the input string to lowercase and then splits it by white spaces\n",
    "\n",
    "Regex tokenizer: A regex based tokenizer extracts tokens by using the provided regex pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'sentence', outputCol = 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(sen_df)\n",
    "tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, sentence='Hi I heard about Spark', words=['hi', 'i', 'heard', 'about', 'spark'])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|     1|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_tokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic,regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol = 'sentence', outputCol = 'words')\n",
    "regex_tokenized = regex_tokenizer.transform(sen_df)\n",
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['hi', 'i', 'heard', 'about', 'spark']),\n",
       " Row(words=['i', 'wish', 'java', 'could', 'use', 'case', 'classes']),\n",
       " Row(words=['logistic', 'regression', 'model', 'are', 'neat'])]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tokenized.select(\"words\").take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\\W matches for any characters which is not a letter, digit or underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic&regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol = 'sentence', outputCol = 'words', pattern = '\\\\W')\n",
    "regex_tokenized = regex_tokenizer.transform(sen_df)\n",
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing tokenizing pattern\n",
    "\n",
    "sen_df = spark.createDataFrame([\n",
    "    (0, 'Hi I heard about Spark'),\n",
    "    (1, 'I wish java could use case classes'),\n",
    "    (2, 'Logistic&regression&model&are&neat')\n",
    "], ['id', 'sentence'])\n",
    "\n",
    "sen_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish java could...|[i, wish, java, c...|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenizer = RegexTokenizer(inputCol = 'sentence', outputCol = 'words', pattern = '\\\\W')\n",
    "regex_tokenized = regex_tokenizer.transform(sen_df)\n",
    "regex_tokenized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['hi', 'i', 'heard', 'about', 'spark']),\n",
       " Row(words=['i', 'wish', 'java', 'could', 'use', 'case', 'classes']),\n",
       " Row(words=['logistic', 'regression', 'model', 'are', 'neat'])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_tokenized.select(\"words\").take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+------+\n",
      "| id|            sentence|               words|tokens|\n",
      "+---+--------------------+--------------------+------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n",
      "|  1|I wish java could...|[i, wish, java, c...|     7|\n",
      "|  2|Logistic,regressi...|[logistic, regres...|     5|\n",
      "+---+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "regex_tokenized.withColumn('tokens', count_tokens(col('words'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are a set of commonly used words in a language. Examples of stop words in English are “a,” “the,” “is,” “are,” etc. Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so widely used that they carry very little useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|              tokens|\n",
      "+---+--------------------+\n",
      "|  0|[I, saw, the, gre...|\n",
      "|  1|[Mary, had, a, li...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0, ['I', 'saw', 'the', 'green', 'horse']),\n",
    "    (1, ['Mary', 'had', 'a', 'little', 'lamb'])\n",
    "], ['id', 'tokens'])\n",
    "\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              tokens|            filtered|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[I, saw, the, gre...| [saw, green, horse]|\n",
      "|  1|[Mary, had, a, li...|[Mary, little, lamb]|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol = 'tokens', outputCol = 'filtered')\n",
    "remover.transform(sentenceDataFrame).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.Ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram is a collection of n successive items in a text document that may include words, numbers, symbols, and punctuation. N-gram models are useful in many text analytics applications where sequences of words are relevant, such as in sentiment analysis, text classification, and text generation. N-gram modeling is one of the many techniques used to convert text from an unstructured format to a structured forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|               words|\n",
      "+---+--------------------+\n",
      "|  0|[Hi, I, heard, ab...|\n",
      "|  1|[I, wish, java, c...|\n",
      "|  2|[Logistic, regres...|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "    (0, ['Hi', 'I', 'heard', 'about', 'Spark']), \n",
    "    (1, ['I', 'wish', 'java', 'could', 'use', 'case', 'classes']),\n",
    "    (2, ['Logistic', 'regression', 'models', 'are', 'neat'])\n",
    "], ['id', 'words'])\n",
    "\n",
    "wordDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|               words|               grams|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|[Hi, I, heard, ab...|[Hi I, I heard, h...|\n",
      "|  1|[I, wish, java, c...|[I wish, wish jav...|\n",
      "|  2|[Logistic, regres...|[Logistic regress...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(inputCol = 'words', outputCol = 'grams')\n",
    "ngram.transform(wordDataFrame).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|grams                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|[Hi I heard about, I heard about Spark]                                              |\n",
      "|[I wish java could, wish java could use, java could use case, could use case classes]|\n",
      "|[Logistic regression models are, regression models are neat]                         |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.transform(wordDataFrame).select('grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------+\n",
      "|grams                                                                                |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|[Hi I heard about, I heard about Spark]                                              |\n",
      "|[I wish java could, wish java could use, java could use case, could use case classes]|\n",
      "|[Logistic regression models are, regression models are neat]                         |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngram.setParams(n=4).transform(wordDataFrame).select('grams').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashing is the process of converting data — text, numbers, files, or anything, really — into a fixed-length string of letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            sentence|\n",
      "+-----+--------------------+\n",
      "|  0.0|Hi I heard about ...|\n",
      "|  0.0|I wish Java could...|\n",
      "|  1.0|Logistic regressi...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame([\n",
    "    (0.0, 'Hi I heard about Spark'), \n",
    "    (0.0, 'I wish Java could use case classes'),\n",
    "    (1.0, 'Logistic regression models are neat')\n",
    "], ['label', 'sentence'])\n",
    "\n",
    "sentenceDataFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|            sentence|               words|\n",
      "+-----+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol = 'sentence', outputCol = 'words')\n",
    "word_df = tokenizer.transform(sentenceDataFrame)\n",
    "word_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(262144,[24417,49...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(262144,[20719,24...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(262144,[13671,91...|\n",
      "+-----+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_tf = HashingTF(inputCol = 'words', outputCol = 'rawFeatures')\n",
    "featurized_df = hashing_tf.transform(word_df)\n",
    "featurized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(rawFeatures=SparseVector(262144, {24417: 1.0, 49304: 1.0, 73197: 1.0, 91137: 1.0, 234657: 1.0}))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurized_df.select(\"rawFeatures\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDF function stands for inverse document frequency and calculates frequency of occurance of a term to the total number of terms in a corpus / collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|            sentence|               words|         rawFeatures|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0.0|Hi I heard about ...|[hi, i, heard, ab...|(262144,[24417,49...|(262144,[24417,49...|\n",
      "|  0.0|I wish Java could...|[i, wish, java, c...|(262144,[20719,24...|(262144,[20719,24...|\n",
      "|  1.0|Logistic regressi...|[logistic, regres...|(262144,[13671,91...|(262144,[13671,91...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf = IDF(inputCol = 'rawFeatures', outputCol = 'features')\n",
    "idf_model = idf.fit(featurized_df)\n",
    "rescaled_df = idf_model.transform(featurized_df)\n",
    "rescaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, sentence='Hi I heard about Spark', words=['hi', 'i', 'heard', 'about', 'spark'], rawFeatures=SparseVector(262144, {24417: 1.0, 49304: 1.0, 73197: 1.0, 91137: 1.0, 234657: 1.0}), features=SparseVector(262144, {24417: 0.2877, 49304: 0.6931, 73197: 0.6931, 91137: 0.6931, 234657: 0.6931}))]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rescaled_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.Count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "| id|          words|\n",
      "+---+---------------+\n",
      "|  0|      [a, b, c]|\n",
      "|  1|[a, b, b, c, a]|\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0, 'a b c'.split(' ')),\n",
    "    (1, 'a b b c a'.split(' '))\n",
    "], ['id', 'words'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol = 'words', outputCol = 'features', vocabSize = 3, minDF = 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+-------------------------+\n",
      "|id |words          |features                 |\n",
      "+---+---------------+-------------------------+\n",
      "|0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n",
      "|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n",
      "+---+---------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_model = cv.fit(df)\n",
    "cv_df = cv_model.transform(df)\n",
    "cv_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark_2_4_5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
