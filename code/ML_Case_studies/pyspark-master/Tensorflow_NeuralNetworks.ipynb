{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-158395db1eb13989",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    " **<font size=5>Programming Assignment 8: Neural Networks with Tensorflow</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c303936544fce875",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Problem Statement\n",
    "\n",
    "In this programming assignment, you will write Tensorflow code to distinguish between a signal process which produces Higgs bosons and a background process which does not. We model this problem as a binary classification problem. \n",
    "\n",
    "Note: This assignment is not designed to make you a professional Tensorflow programmer, but rather to introduce you to, and make you practice, the basic constructs and functionalities of Tensorflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cedd227ae71fb17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### CPU vs GPU\n",
    "\n",
    "You may want to read [this article](https://www.analyticsvidhya.com/blog/2017/05/gpus-necessary-for-deep-learning/) to know more about the CPU vs GPU discussion. This is totally optional, still highly recommended for those who are interested. \n",
    "\n",
    "You do not need to write any \"GPU specific\" code. Tensorflow automatically recognizes the underlying hardware, and optimizes your code to run accordingly.\n",
    "\n",
    "The most common bottleneck to training faster with a GPU is usually the speed at which data is fed to the GPU for processing. So the input data pipeline is an important construct when writing efficient, scalable code to train Neural networks using Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f87b642b5252980b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Dataset\n",
    "\n",
    "For this assignment, we will use sampled data from a well known dataset: [Higgs Dataset](https://archive.ics.uci.edu/ml/datasets/HIGGS). \n",
    "\n",
    "Some information regarding the data and the problem: \n",
    "\n",
    "This is a classification problem to distinguish between a signal process which produces Higgs bosons and a background process which does not. The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f1b9f41248e8378",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The train and test files have the following characteristics:\n",
    "\n",
    "- The first row is a header that contains a comma-separated list of the names of the label and attributes\n",
    "- Each successive row represents a single example\n",
    "- The first column of each example is the label to be learned, and all other columns are attribute values.\n",
    "- All attributes are numerical i.e. real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc620c75541dd80e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Testing and Evaluation\n",
    "\n",
    "\n",
    "For local testing and for two of the three submissions, you will use the small training and test datasets that have been provided along with this notebook. \n",
    "\n",
    "When submitting on EdX, your code will be trained and evaluated on a much larger sample of the full dataset. \n",
    "\n",
    "Some suggestions you should keep in mind while implementing the functions:\n",
    "\n",
    "- Avoid doing repeated work i.e. anything that could be done outside the loop, should be outside.\n",
    "- Read the markdown of this notebook carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6995099148a8d8ee",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Configuration File\n",
    "\n",
    "To make your code more robust, and to aid the process of grading your work, most of the required parameters for the network training and testing will come from a [YAML](http://yaml.org/) config file named \"nn_config.yaml\". \n",
    "\n",
    "This file is present in the same directory as the notebook. We have added default values to parameters, but you may modify them for debugging purposes. \n",
    "\n",
    "Information regarding what these variables mean, and how you should use them is present as comments in the yaml file. Information regarding how to read variables from the YAML config file is mentioned later in this notebook.\n",
    "\n",
    "However, remember that for grading your work we will use our own config files. So your code should always refer to variable values from the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:06.573599Z",
     "start_time": "2018-12-05T04:40:06.289922Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0e011b46c2ce211f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Training data file path\r\n",
      "training_data_path: /home/ubuntu/HW8/Data/higgs_train_large.csv\r\n",
      "\r\n",
      "## Testing data file path\r\n",
      "test_data_path: /home/ubuntu/HW8/Data/higgs_test_large.csv \r\n",
      "\r\n",
      "## Location in which you will save the pickle file containing predictions on test data\r\n",
      "output_predictions_pickle_path: ./test_predictions.pkl\r\n",
      "\r\n",
      "## How to split the input training data into train and validation sets. Value of 0.8 means that if there were initially 100 training examples in the input data, you should split them such that the first 80 are used as training examples and last 20 should be used for validation.\r\n",
      "training_to_validation_ratio: 0.8\r\n",
      "\r\n",
      "## the learning rate you should use for your optimizer \r\n",
      "learning_rate: 0.05\r\n",
      "\r\n",
      "## the total number of epochs or iterations to run over the (80) training examples\r\n",
      "epochs: 200\r\n",
      "\r\n",
      "## the number of mini batches in which you should split your training examples. Continuing with our example, if this value is 10, then each mini batch will have 8 training examples.\r\n",
      "num_mini_batches: 5\r\n",
      "\r\n",
      "## this variable is for your own use to modify when you would like to print any debug statements inside your training loop. For example, a value of 5 may suggest that you should print the training loss, validation loss, training accuracy and validation accuracy every 5 epochs.\r\n",
      "display_step: 1\r\n",
      "\r\n",
      "## the size of the network. If first_layer: 20 and second_layer: 8 then you should set the number of hidden units in first hidden layer and second layer to be 20 and 8 respectively.\r\n",
      "hidden_layer_sizes: \r\n",
      "    first_layer: 20\r\n",
      "    second_layer: 8\r\n",
      "\r\n",
      "## for grading purposes, you may ignore\r\n",
      "dataset_size: large\r\n",
      "grading_script_path: /home/ubuntu/HW8/grade_test_submission.py"
     ]
    }
   ],
   "source": [
    "# Let's look at the contents of the YAML config file\n",
    "!cat Data/nn_config.yaml "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a4e23526701b920",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Gameplan\n",
    "\n",
    "You will write robust code that builds a feedforward neural network, and trains it according to the given set of parameters.\n",
    "\n",
    "1. We will first load the training and test data using the parameters from the config file.\n",
    "2. We will then split the training data into training and validation sets using the value of \"training_to_validation_ratio\" parameter in the config. \n",
    "    For example, if the param is 0.8, it means that the initial 80% of the data should be kept for training, while the rest 20% should be used for validation.\n",
    "3. We will use Cross Entropy Loss as our cost functions and minimize it using AdamOptimizer as our optimizer.\n",
    "4. We will train our model in batches inside our main training loop. You will divide the training data into `num_batches` number of mini batches and for each epoch you will iterate and train over those many number of batches.\n",
    "5. You can use \"display_step\" param to control the frequency of print statements.\n",
    "6. You will maintain a list of training accuracies and losses (one value for each epoch).\n",
    "7. You will maintain a list of validation accuracy and loss (one value for each epoch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c5dc5f0d1795d09a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "\n",
    "    The function tf.reduce_sum will allow you to sum across all instances.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2952f5d422d0d1e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "5) You should train your network using your inputted learning rate and for the inputted number of iterations. The iterations are simply a loop that calls Backpropagation a fixed number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:06.577749Z",
     "start_time": "2018-12-05T04:40:06.575289Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c01b1c216ee0572",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Tensorflow produces a lot of warnings. We generally want to suppress them. The below code does exactly that. \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.505370Z",
     "start_time": "2018-12-05T04:40:06.579248Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a554a48f73f64b13",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "## Pretty Print\n",
    "import pprint as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.511034Z",
     "start_time": "2018-12-05T04:40:12.506754Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1e7a22edc0fe397",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "def import_config():\n",
    "    with open(\"nn_config.yaml\", 'r') as ymlfile:\n",
    "        try:\n",
    "            cfg = yaml.load(ymlfile)\n",
    "        except yaml.YAMLError as err:\n",
    "            print(err)\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.604245Z",
     "start_time": "2018-12-05T04:40:12.512361Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbb7c3cab0be00a7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your hardware either does not have a GPU or is not configured to use the GPU version of TF.\n",
      "        However, you do not need a GPU for this assignment as you will be completing this assigment on a\n",
      "        CPU enviroment, but evaluating it on a GPU enviroment.\n"
     ]
    }
   ],
   "source": [
    "if 'session' in locals() and session is not None:\n",
    "    print('Close interactive session')\n",
    "    session.close()\n",
    "\n",
    "## The below function tests if Tensorflow has access to GPU or not.\n",
    "def test_cpu_gpu():\n",
    "    if tf.test.gpu_device_name():\n",
    "        print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "    else:\n",
    "        print('''Your hardware either does not have a GPU or is not configured to use the GPU version of TF.\n",
    "        However, you do not need a GPU for this assignment as you will be completing this assigment on a\n",
    "        CPU enviroment, but evaluating it on a GPU enviroment.''')\n",
    "        \n",
    "test_cpu_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.688565Z",
     "start_time": "2018-12-05T04:40:12.605558Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a59bb5add2fa75d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_size': 'large',\n",
      " 'display_step': 1,\n",
      " 'epochs': 200,\n",
      " 'grading_script_path': '/home/ubuntu/HW8/grade_test_submission.py',\n",
      " 'hidden_layer_sizes': {'first_layer': 20, 'second_layer': 8},\n",
      " 'learning_rate': 0.05,\n",
      " 'num_mini_batches': 5,\n",
      " 'output_predictions_pickle_path': './test_predictions.pkl',\n",
      " 'test_data_path': '/home/jana/edx/pa8-fixed/Data/higgs_test_large.csv',\n",
      " 'training_data_path': '/home/jana/edx/pa8-fixed/Data/higgs_train_large.csv',\n",
      " 'training_to_validation_ratio': 0.8}\n"
     ]
    }
   ],
   "source": [
    "cfg = import_config()\n",
    "\n",
    "## Is it loaded correctly?\n",
    "pp.pprint(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.737709Z",
     "start_time": "2018-12-05T04:40:12.689844Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-855db077a3fa0f0b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "train_file_name = cfg['training_data_path']\n",
    "test_file_name = cfg['test_data_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:12.787670Z",
     "start_time": "2018-12-05T04:40:12.739115Z"
    }
   },
   "outputs": [],
   "source": [
    "# =========================================================================================== #\n",
    "# Uncomment this to test on smaller dataset. This is faster and can be used to debug quickly. #\n",
    "# PLEASE COMMENT THIS BEFORE SUBMITTING. YOUR NOTEBOOK IS EVALUATED ON LARGE DATASET.         #\n",
    "# =========================================================================================== #\n",
    "\n",
    "train_file_name = 'Data/higgs_train_small.csv'\n",
    "test_file_name = 'Data/higgs_test_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.256233Z",
     "start_time": "2018-12-05T04:40:12.789007Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6fc5a4686fa8347",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Loading the Data\n",
    "training_data = np.loadtxt(train_file_name, delimiter = ',')\n",
    "test_data = np.loadtxt(test_file_name, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f6ae15f9a3523674",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now we have loaded the training and test data. However, we cannot use it directly. We first need to standardize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb592faa9bc02c71",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Implement the Standardize Function \n",
    "\n",
    "Neural networks work best when all features roughly are on the same scale and are centered around the mean.\n",
    "\n",
    "This is done by standardizing the feature vectors. Feature standardization makes the values of each feature in the data have zero-mean (when subtracting the mean in the numerator) and unit-variance.\n",
    "\n",
    "The function <font color=\"blue\">standardize</font> takes the input data and determines the distribution mean and standard deviation for each feature. Next the mean is subtracted from each feature. Then the mean-subtracted values of each feature are divided by its standard deviation.\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Input</font>**\n",
    "There are 3 training examples with 4 features each \n",
    "``` python\n",
    "np.array([[-0.22 -0.19 -0.17 -0.13][-0.1 -0.05 0.02 0.10][0.03 0.11 0.12 0.15]])\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "There are 3 training examples (which have been standardized) along each of the 4 features\n",
    "``` python\n",
    "array([[-1.20809282, -1.19664225, -1.33025759, -1.39425471],\n",
    "       [-0.03265116, -0.05439283,  0.2494233 ,  0.4920899 ],\n",
    "       [ 1.24074398,  1.25103507,  1.08083429,  0.90216481]])\n",
    "```\n",
    "\n",
    "Refer the \"Standardization\" section of this [Wikipedia Feature Scaling Article](https://en.wikipedia.org/wiki/Feature_scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.260711Z",
     "start_time": "2018-12-05T04:40:13.257567Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-305b411920b596a3",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def standardize(data):\n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    return (data - np.mean(data, axis=0)) / np.std(data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.329333Z",
     "start_time": "2018-12-05T04:40:13.262627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.20809282 -1.19664225 -1.33025759 -1.39425471]\n",
      " [-0.03265116 -0.05439283  0.2494233   0.4920899 ]\n",
      " [ 1.24074398  1.25103507  1.08083429  0.90216481]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[-0.22, -0.19, -0.17, -0.13],[-0.1, -0.05, 0.02, 0.10],[0.03, 0.11, 0.12, 0.15]])\n",
    "print(standardize(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.372432Z",
     "start_time": "2018-12-05T04:40:13.330674Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "qdummy = np.array([2,1]*10)\n",
    "print(np.round(standardize(qdummy)[1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.414527Z",
     "start_time": "2018-12-05T04:40:13.373786Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "standardize_vt",
     "locked": true,
     "points": "5",
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dummy = np.array([[-0.22, -0.19, -0.17, -0.13],[-0.1, -0.05, 0.02, 0.10],[0.03, 0.11, 0.12, 0.15]])\n",
    "assert standardize(dummy).__class__ == np.ndarray, \"should return numpy array\"\n",
    "assert standardize(dummy).shape == dummy.shape, \"should have the same shape as the input array\"\n",
    "\n",
    "dummy_ans = np.round(np.array([[-1.20809282, -1.19664225, -1.33025759, -1.39425471],\n",
    "       [-0.03265116, -0.05439283,  0.2494233 ,  0.4920899 ],\n",
    "       [ 1.24074398,  1.25103507,  1.08083429,  0.90216481]]),3)\n",
    "assert (np.round(standardize(dummy)[0],3)==dummy_ans[0]).all(), \"check for correct return value failed\"\n",
    "assert (np.round(standardize(dummy)[2],3)==dummy_ans[2]).all(), \"check for correct return value failed\"\n",
    "del dummy, dummy_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.463275Z",
     "start_time": "2018-12-05T04:40:13.415761Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "standardize_ht",
     "locked": true,
     "points": "10",
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hidden Tests Here\n",
    "\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6d1d2ce4cf36f9c1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Implement the parse_training_data function\n",
    "\n",
    "The function <font color=\"blue\">parse_training_data</font> takes the input data and returns labels and features. \n",
    "\n",
    "Remember that the first column of the training data is the labels, and the remaining columns are the features\n",
    "\n",
    "The labels should be reshaped to a 2-D numpy matrix of shape (dataset_size, 1)\n",
    "The features should be standardized and have be a 2-D numpy matrix of shape (dataset_size, 28)\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Input</font>**\n",
    "There are 3 training examples with the label and 3 features each \n",
    "``` python\n",
    "np.array([[1 -0.19 -0.17 -0.13][0 -0.05 0.02 0.10][0 0.11 0.12 0.15]])\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "Returns a tuple:\n",
    "    1st element is the labels\n",
    "    2nd element is the standardized features\n",
    "``` python\n",
    "(array([[1.],\n",
    "        [0.],\n",
    "        [1.]]), array([[-1.4688735 , -1.3105518 , -0.99390842],\n",
    "        [-0.36062164,  0.19350429,  0.82679107],\n",
    "        [ 0.90595192,  0.98511277,  1.22259531]]))\n",
    "```\n",
    "\n",
    "\n",
    "Remember to use the standardize function appropriately inside this function and use the visible assert statements to finetune the shape of your returned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.505078Z",
     "start_time": "2018-12-05T04:40:13.464588Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b74473c65b9d3782",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_training_data(numpy_matrix):\n",
    "    \n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    dataset_size, nbr_cols = numpy_matrix.shape\n",
    "    labels = numpy_matrix[:,0].reshape(dataset_size,1)\n",
    "    features = standardize(numpy_matrix[:,1:nbr_cols].reshape(dataset_size, nbr_cols - 1))\n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.551172Z",
     "start_time": "2018-12-05T04:40:13.506395Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0e069072154f863",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Parse Training Data. You will later split the `labels` and `features` into training and validation sets.\n",
    "labels, features = parse_training_data(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.589227Z",
     "start_time": "2018-12-05T04:40:13.552423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape =  (10000, 1) , features shape =  (10000, 28)\n"
     ]
    }
   ],
   "source": [
    "print(\"labels shape = \",labels.shape,\", features shape = \", features.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.638404Z",
     "start_time": "2018-12-05T04:40:13.590443Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "shape_v",
     "locked": true,
     "points": "5",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert labels.shape[1] == 1\n",
    "assert features.shape[1] == 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.679934Z",
     "start_time": "2018-12-05T04:40:13.639654Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "values_h",
     "locked": true,
     "points": "10",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-714a63065a14a65f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Implement the parse_test_data function\n",
    "\n",
    "The function <font color=\"blue\">parse_test_data</font> takes the input data and returns labels and features. \n",
    "\n",
    "We do not have access to labels while predicting the classes that our test examples belong to. The input data files for the test data would not have the labels column. \n",
    "\n",
    "So we need a different function to parse the test data. This should only return standardized features.\n",
    "\n",
    "The features should be standardized and have be a 2-D numpy matrix of shape (dataset_size, 28)\n",
    "\n",
    "**<font color=\"magenta\" size=2>Example Input</font>**\n",
    "There are 3 training examples with the label and 3 features each \n",
    "``` python\n",
    "np.array([[-0.19 -0.17 -0.13][-0.05 0.02 0.10][0.11 0.12 0.15]])\n",
    "```\n",
    "\n",
    "**<font color=\"blue\" size=2>Example Output</font>**\n",
    "Returns a tuple:\n",
    "    1st element is the labels\n",
    "    2nd element is the standardized features\n",
    "``` python\n",
    "array([[-1.4688735 , -1.3105518 , -0.99390842],\n",
    "        [-0.36062164,  0.19350429,  0.82679107],\n",
    "        [ 0.90595192,  0.98511277,  1.22259531]])\n",
    "```\n",
    "\n",
    "\n",
    "Remember to use the standardize function appropriately inside this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.738340Z",
     "start_time": "2018-12-05T04:40:13.681330Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd58458c52559f0a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_test_data(numpy_matrix):\n",
    "    \n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    dataset_size, nbr_cols = numpy_matrix.shape\n",
    "    #labels = numpy_matrix[:,0].reshape(dataset_size,1)\n",
    "    test_features = standardize(numpy_matrix[:,0:nbr_cols].reshape(dataset_size, nbr_cols))\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.806397Z",
     "start_time": "2018-12-05T04:40:13.739569Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-febfb665f2f27e1e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_features = parse_test_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.872957Z",
     "start_time": "2018-12-05T04:40:13.807739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 28)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0de20ca899b3afe6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Building the Neural Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6b3f768be0607ffd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Initializing important parameters\n",
    "Use the below params appropriately inside the train_nn() function. We have initialized these variables in order to assist you in your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:13.922007Z",
     "start_time": "2018-12-05T04:40:13.874197Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9496cea12852d1fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training examples: 8000, Number of Batches: 5, Batch Size: 1600\n"
     ]
    }
   ],
   "source": [
    "learning_rate = cfg['learning_rate']\n",
    "training_epochs = cfg['epochs']\n",
    "train_valid_split = cfg['training_to_validation_ratio']\n",
    "num_batches = cfg['num_mini_batches']\n",
    "display_step = cfg['display_step']\n",
    "\n",
    "num_examples= training_data.shape[0]\n",
    "\n",
    "# The first `num_train_examples` should be used for training, the rest for validation.\n",
    "num_train_examples = int(num_examples * train_valid_split)\n",
    "\n",
    "batch_size = num_train_examples/num_batches\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = cfg['hidden_layer_sizes']['first_layer'] # 1st layer number of features\n",
    "n_hidden_2 = cfg['hidden_layer_sizes']['second_layer'] # 2nd layer number of features\n",
    "n_input = 28 \n",
    "n_classes = 1 \n",
    "\n",
    "\n",
    "print(\"Total Training examples: %d, Number of Batches: %d, Batch Size: %d\" %(num_train_examples,num_batches,batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dc994f1830e0a244",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Initializing placeholders for feeding into the TF graph\n",
    "\n",
    "Define the TF placeholders which will receive data for each mini batch. Similarly define weights and biases as TF variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.217465Z",
     "start_time": "2018-12-05T04:40:13.923360Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-47aa2936b36e30b8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# TF Graph input\n",
    "## Use the below placeholders appropriately inside the train_nn() function\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-714a63065a14a6sd5f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Implement the `calc_num_total_learnable_params` function.\n",
    "\n",
    "This function  calculates the number of learnable parameters of the network model. This number directly relates to the complexity of your model, as well as the training time. \n",
    "\n",
    "The function <font color=\"blue\">calc_num_total_learnable_params</font> takes the weights dictionary and bias dictionary and returns an integer which is equal to the number of total parameters in the network. \n",
    "\n",
    "You can make use of the `get_dims_as_tuple` as a helper function to access the shape of the weight and bias matrices easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.227141Z",
     "start_time": "2018-12-05T04:40:14.219110Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bb3e6250332e0033",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 20)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function which you may use in implementing `calc_num_total_learnable_params(weights,biases)` function below.\n",
    "def get_dims_as_tuple(x):\n",
    "    shape = x.get_shape()\n",
    "    dims = []\n",
    "    for dim in shape:\n",
    "        dims.append(dim.value)\n",
    "    return tuple(dims)\n",
    "\n",
    "# example usage:\n",
    "get_dims_as_tuple(weights['h1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.298772Z",
     "start_time": "2018-12-05T04:40:14.229005Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-94e7366afdd6358a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def calc_num_total_learnable_params(weights,biases):\n",
    "    \n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    w1,w2 = get_dims_as_tuple(weights)\n",
    "    b1,b2 = get_dims_as_tuple(biases)\n",
    "    return w1*w2 + b1*b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.357561Z",
     "start_time": "2018-12-05T04:40:14.300491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'h1': <tf.Variable 'Variable:0' shape=(28, 20) dtype=float32_ref>, 'h2': <tf.Variable 'Variable_1:0' shape=(20, 8) dtype=float32_ref>, 'out': <tf.Variable 'Variable_2:0' shape=(8, 1) dtype=float32_ref>}\n",
      "{'b1': <tf.Variable 'Variable_3:0' shape=(20,) dtype=float32_ref>, 'b2': <tf.Variable 'Variable_4:0' shape=(8,) dtype=float32_ref>, 'out': <tf.Variable 'Variable_5:0' shape=(1,) dtype=float32_ref>}\n",
      "757\n"
     ]
    }
   ],
   "source": [
    "#print(calc_num_total_learnable_params(weights,biases))\n",
    "print(weights)\n",
    "print(biases)\n",
    "print(28 * 20 + 20 * 8 + 8 + 20 + 8 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.397483Z",
     "start_time": "2018-12-05T04:40:14.358797Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "learnable_ht",
     "locked": true,
     "points": "10",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Hidden Tests Here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-40285d4b5a7d0481",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Create FeedForward Network Model\n",
    "\n",
    "This function needs to be filled up with code to construct the remaining two layers of the neural network. You have to add one more hidden layers and also the output layer.\n",
    "\n",
    "You should use the sigmoid activation function. Tensorflow's `tf.nn.sigmoid()` function should be helpful. \n",
    "\n",
    "We have partially implemented this function. Complete the rest of it. Remember to not apply the sigmoid activation at the last layer as we will be using `tf.nn.sigmoid_cross_entropy_with_logits()` later which does that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.447919Z",
     "start_time": "2018-12-05T04:40:14.398856Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fd6390a18e16110",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_feedforward_nn_model(x, weights, biases):\n",
    "    # Hidden layer with SIGMOID activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.sigmoid(layer_1)\n",
    "    \n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.sigmoid(layer_2)\n",
    "\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    \n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6fcb5f53e1dbbae9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Stitch the Neural Network Model\n",
    "\n",
    "Using the appropriate Tensorflow libraries, implement each of the following operations:\n",
    "- loss as the CrossEntropyLoss\n",
    "- train_op as the AdamOptimizer that minimizes the loss\n",
    "\n",
    "As inputs to these operators, you can use:\n",
    "- `pred_raw` which is the output of your neural network's last layer\n",
    "- `pred` is the predicted label, which is the output of rounding `pred_raw`. \n",
    "\n",
    "You might want to look at the Tensorflow Section notebooks as well as the TensorFlow API.\n",
    "\n",
    "Two of the returned values have been implemented as a hint for you.\n",
    "\n",
    "Functions that could be useful here: \n",
    "\n",
    "```python\n",
    "tf.nn.sigmoid_cross_entropy_with_logits()\n",
    "tf.reduce_mean()\n",
    "tf.round()\n",
    "tf.sigmoid()\n",
    "tf.train.AdamOptimizer().minimize()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.846861Z",
     "start_time": "2018-12-05T04:40:14.449319Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2a697316f54fa391",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "def stitch_network(x, y, weights, biases, learning_rate):\n",
    "    \n",
    "    pred_raw = create_feedforward_nn_model(x, weights, biases)\n",
    "    pred = tf.round(tf.nn.sigmoid(pred_raw))\n",
    "    \n",
    "    #\n",
    "    # YOUR CODE HERE\n",
    "    #\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred_raw, labels=y))\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    return pred_raw, pred, cost, train_op\n",
    "\n",
    "\n",
    "pred_raw, pred, cost, train_op = stitch_network(x, y, weights, biases, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.850949Z",
     "start_time": "2018-12-05T04:40:14.848362Z"
    },
    "nbgrader": {
     "grade": true,
     "grade_id": "shape2_v",
     "locked": true,
     "points": "5",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert cost.__class__ == tf.Tensor\n",
    "assert cost.get_shape() == (), \"Make sure you have used reduce_mean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T04:40:14.906450Z",
     "start_time": "2018-12-05T04:40:14.852389Z"
    },
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9b4c53caccf6e1d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Initializing the variables - IMPORTANT\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b683e70b1b3eafa0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Training and Testing the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b683e70b1b3asdaeafa0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Exercise: Writing the Train function\n",
    "\n",
    "This is where you will train your network.\n",
    "\n",
    "Your goal is to complete the following function named `train_nn()`. \n",
    "\n",
    "To help you structure your implementation, we have provided some starter code. \n",
    "\n",
    "We have also detailed each of the steps you need to pay attention to inside the main training loop.\n",
    "\n",
    "Remember you have access to all the parameters we initialized early on in the notebook, as well as to the parameters defined in the config file. \n",
    "\n",
    "`train_nn()` should return 5 python lists\n",
    "1. training_costs\n",
    "2. validation_costs\n",
    "3. training_accs\n",
    "4. validation_accs\n",
    "5. test_predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd22bec67d6afd31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_nn():\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        \n",
    "        ## this is needed to print debug statements during training.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        x_train, x_valid = features[:num_train_examples], features[num_train_examples:]\n",
    "        y_train, y_valid = labels[:num_train_examples], labels[num_train_examples:]\n",
    "\n",
    "        training_costs = []\n",
    "        training_accs = []\n",
    "\n",
    "        validation_costs = []\n",
    "        validation_accs = []\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "        \n",
    "            '''\n",
    "            We recommend you first think about how you will implement this on your own before proceeding to read any further.\n",
    "        \n",
    "            HINT: You should implement the following procedure here:\n",
    "        \n",
    "            An epoch is one pass through your training data\n",
    "            1. Keep a counter of your epoch's total cost. \n",
    "               You will need this to average over the batches.\n",
    "            2. Keep a counter of the number of correct predictions in your epoch. \n",
    "               You will need this to sum over the batches to calculate per epoch accuracy.\n",
    "            \n",
    "            For each batch  (you should have `num_batches` number of batches totally)\n",
    "                --Batchwise training--\n",
    "                3. subset your features and labels from x_train and y_train\n",
    "                ex. for batch 1, you'd select all examples in the interval [0,batch_size)\n",
    "                    for batch 2, it would be between [batch_size, 2*batch_size)\n",
    "                    Make sure to account for a possible fractional batch as your last batch\n",
    "                4. Massage your x_batch and y_batch into numpy arrays of shape (size_of_batch, 28) \n",
    "                    and (size_of_batch, 1) respectively\n",
    "                    \n",
    "                5. Feed the x_batch and y_batch into your tensorflow graph and execute the optimizer, cost and pred \n",
    "                    in order to train your model using the current batch and also get back the batch_cost and batch_predictions\n",
    "                6. Count the number of correct predictions for this batch and add it to the counter for the \n",
    "                    correct predictions in the epoch\n",
    "                    \n",
    "            7. Calculate your average_epoch_cost as the total_epoch_cost divided by the number of training examples\n",
    "            8. Append the average_epoch_cost to `training_costs`\n",
    "            9. Calculate your epoch_accuracy as the total number of correct predictions in your epoch\n",
    "                    divided by the number of training examples\n",
    "            10. Append the epoch_accuracy to `training_accs`\n",
    "            \n",
    "            --Validation--\n",
    "            \n",
    "            11. Massage your validation labels (y_valid) into a numpy arrays of shape (validation_set_size, 1)\n",
    "            12. With y_valid and x_valid as input to your graph, calculate the validation loss and validation predictions\n",
    "                    We are calculating validation accuracy at the end of each epoch\n",
    "            13. Calculate the number of correct validation predictions by comparing against your validation labels\n",
    "            14. Append validation costs and validation accuracy to their respective lists\n",
    "            15. Avoid printing a lot of debug information when you submit the assignment. \n",
    "                This reduces the speed of execution.\n",
    "                If you want to print some information every so often, you can use the following line \n",
    "                at the end of your epoch loop:\n",
    "                if epoch%display_step==0:\n",
    "                    print(\"Epoch %d | Tr cost: %f | Tr accuracy %f | Va cost: %f | Va accuracy: \n",
    "                        %f\"%(epoch + 1,avg_epoch_cost, this_epoch_accuracy, batch_valid_cost, valid_accuracy))\n",
    "            '''\n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        ##1.counter of your epoch's total cost\n",
    "        total_epoch_cost = 0\n",
    "        ##2.counter of the number of correct predictions in your epoch\n",
    "        correct_pred_count = 0\n",
    "        \n",
    "        batch_size = int(num_examples / num_batches)\n",
    "        \n",
    "        ##For each batch  (you should have `num_batches` number of batches totally)\n",
    "        for i in range(num_batches):\n",
    "            ##3.subset your features and labels from x_train and y_train\n",
    "            batch_start = i * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            if (batch_end > num_examples):\n",
    "                batch_end = num_examples\n",
    "            batch_xs, batch_ys = x_train[batch_start:batch_end,:], y_train[batch_start:batch_end,:]\n",
    "\n",
    "            ##5. Feed the x_batch and y_batch into your tensorflow graph and execute the optimizer, cost and pred \n",
    "            ##  in order to train your model using the current batch and also get back the batch_cost and batch_predictions            \n",
    "            pred_raw, pred, cost, train_op = stitch_network(batch_xs, batch_ys, weights, biases, learning_rate)\n",
    "            = sess.run(train_op, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "            ##6. Count the number of correct predictions for this batch and add it to the counter for the \n",
    "            ##  correct predictions in the epoch\n",
    "\n",
    "            \n",
    "        print(\"Optimization Finished!\")\n",
    "\n",
    "        ## Assuming the above part is completed, you should now use your trained model to make predictions on the test set.\n",
    "        test_predictions = []\n",
    "        \n",
    "        #\n",
    "        # YOUR CODE HERE\n",
    "        #\n",
    "        \n",
    "        \n",
    "        ## this is needed to print debug statements during training.\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "    \n",
    "    ## close TF session if open\n",
    "    if 'session' in locals() and sess is not None:\n",
    "        print('Close interactive session')\n",
    "        sess.close()\n",
    "        \n",
    "    return training_costs, validation_costs, training_accs, validation_accs, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-099ff13f7570ad44",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Tr cost: 0.881134 | Tr accuracy 0.519840 | Va cost: 0.773548 | Va accuracy: 0.503125\n",
      "Epoch 2 | Tr cost: 0.747966 | Tr accuracy 0.515340 | Va cost: 0.699275 | Va accuracy: 0.546335\n",
      "Epoch 3 | Tr cost: 0.690124 | Tr accuracy 0.561046 | Va cost: 0.682865 | Va accuracy: 0.577695\n",
      "Epoch 4 | Tr cost: 0.678405 | Tr accuracy 0.583925 | Va cost: 0.669029 | Va accuracy: 0.594550\n",
      "Epoch 5 | Tr cost: 0.664657 | Tr accuracy 0.599519 | Va cost: 0.660138 | Va accuracy: 0.607295\n",
      "Epoch 6 | Tr cost: 0.659124 | Tr accuracy 0.608936 | Va cost: 0.656126 | Va accuracy: 0.617205\n",
      "Epoch 7 | Tr cost: 0.654016 | Tr accuracy 0.617735 | Va cost: 0.651585 | Va accuracy: 0.620555\n",
      "Epoch 8 | Tr cost: 0.650534 | Tr accuracy 0.621211 | Va cost: 0.648063 | Va accuracy: 0.626025\n",
      "Epoch 9 | Tr cost: 0.646462 | Tr accuracy 0.630044 | Va cost: 0.644410 | Va accuracy: 0.635485\n",
      "Epoch 10 | Tr cost: 0.642746 | Tr accuracy 0.637826 | Va cost: 0.640565 | Va accuracy: 0.640150\n",
      "Epoch 11 | Tr cost: 0.638901 | Tr accuracy 0.641860 | Va cost: 0.637207 | Va accuracy: 0.644460\n",
      "Epoch 12 | Tr cost: 0.635480 | Tr accuracy 0.647744 | Va cost: 0.634189 | Va accuracy: 0.649125\n",
      "Epoch 13 | Tr cost: 0.632411 | Tr accuracy 0.651807 | Va cost: 0.631289 | Va accuracy: 0.652400\n",
      "Epoch 14 | Tr cost: 0.629532 | Tr accuracy 0.655374 | Va cost: 0.628613 | Va accuracy: 0.655930\n",
      "Epoch 15 | Tr cost: 0.626853 | Tr accuracy 0.658589 | Va cost: 0.626065 | Va accuracy: 0.659070\n",
      "Epoch 16 | Tr cost: 0.624280 | Tr accuracy 0.661370 | Va cost: 0.623637 | Va accuracy: 0.661600\n",
      "Epoch 17 | Tr cost: 0.621863 | Tr accuracy 0.664196 | Va cost: 0.621311 | Va accuracy: 0.663680\n",
      "Epoch 18 | Tr cost: 0.619448 | Tr accuracy 0.666650 | Va cost: 0.618901 | Va accuracy: 0.666045\n",
      "Epoch 19 | Tr cost: 0.616941 | Tr accuracy 0.669512 | Va cost: 0.616304 | Va accuracy: 0.668765\n",
      "Epoch 20 | Tr cost: 0.614237 | Tr accuracy 0.672488 | Va cost: 0.613423 | Va accuracy: 0.671930\n",
      "Epoch 21 | Tr cost: 0.611289 | Tr accuracy 0.675396 | Va cost: 0.610216 | Va accuracy: 0.675785\n",
      "Epoch 22 | Tr cost: 0.607975 | Tr accuracy 0.678609 | Va cost: 0.606592 | Va accuracy: 0.679115\n",
      "Epoch 23 | Tr cost: 0.604249 | Tr accuracy 0.681576 | Va cost: 0.602685 | Va accuracy: 0.682610\n",
      "Epoch 24 | Tr cost: 0.600517 | Tr accuracy 0.684494 | Va cost: 0.599270 | Va accuracy: 0.685090\n",
      "Epoch 25 | Tr cost: 0.597337 | Tr accuracy 0.686943 | Va cost: 0.596354 | Va accuracy: 0.687670\n",
      "Epoch 26 | Tr cost: 0.594456 | Tr accuracy 0.689200 | Va cost: 0.593530 | Va accuracy: 0.690020\n",
      "Epoch 27 | Tr cost: 0.591588 | Tr accuracy 0.691532 | Va cost: 0.590698 | Va accuracy: 0.692090\n",
      "Epoch 28 | Tr cost: 0.588682 | Tr accuracy 0.694066 | Va cost: 0.587838 | Va accuracy: 0.693755\n",
      "Epoch 29 | Tr cost: 0.585735 | Tr accuracy 0.696427 | Va cost: 0.584957 | Va accuracy: 0.695985\n",
      "Epoch 30 | Tr cost: 0.582804 | Tr accuracy 0.698335 | Va cost: 0.582115 | Va accuracy: 0.697770\n",
      "Epoch 31 | Tr cost: 0.579943 | Tr accuracy 0.700166 | Va cost: 0.579332 | Va accuracy: 0.699770\n",
      "Epoch 32 | Tr cost: 0.577142 | Tr accuracy 0.701630 | Va cost: 0.576595 | Va accuracy: 0.701400\n",
      "Epoch 33 | Tr cost: 0.574430 | Tr accuracy 0.703290 | Va cost: 0.573953 | Va accuracy: 0.702925\n",
      "Epoch 34 | Tr cost: 0.571838 | Tr accuracy 0.704836 | Va cost: 0.571490 | Va accuracy: 0.704340\n",
      "Epoch 35 | Tr cost: 0.569418 | Tr accuracy 0.706474 | Va cost: 0.569310 | Va accuracy: 0.706115\n",
      "Epoch 36 | Tr cost: 0.567211 | Tr accuracy 0.707769 | Va cost: 0.567478 | Va accuracy: 0.707125\n",
      "Epoch 37 | Tr cost: 0.565260 | Tr accuracy 0.708830 | Va cost: 0.565927 | Va accuracy: 0.707880\n",
      "Epoch 38 | Tr cost: 0.563582 | Tr accuracy 0.709561 | Va cost: 0.564446 | Va accuracy: 0.708645\n",
      "Epoch 39 | Tr cost: 0.562131 | Tr accuracy 0.710229 | Va cost: 0.562946 | Va accuracy: 0.709245\n",
      "Epoch 40 | Tr cost: 0.560838 | Tr accuracy 0.711060 | Va cost: 0.561564 | Va accuracy: 0.709975\n",
      "Epoch 41 | Tr cost: 0.559675 | Tr accuracy 0.711916 | Va cost: 0.560452 | Va accuracy: 0.710510\n",
      "Epoch 42 | Tr cost: 0.558661 | Tr accuracy 0.712501 | Va cost: 0.559564 | Va accuracy: 0.710910\n",
      "Epoch 43 | Tr cost: 0.557776 | Tr accuracy 0.712879 | Va cost: 0.558808 | Va accuracy: 0.711175\n",
      "Epoch 44 | Tr cost: 0.556954 | Tr accuracy 0.713436 | Va cost: 0.558044 | Va accuracy: 0.711615\n",
      "Epoch 45 | Tr cost: 0.556129 | Tr accuracy 0.713878 | Va cost: 0.557206 | Va accuracy: 0.712220\n",
      "Epoch 46 | Tr cost: 0.555299 | Tr accuracy 0.714336 | Va cost: 0.556399 | Va accuracy: 0.712885\n",
      "Epoch 47 | Tr cost: 0.554471 | Tr accuracy 0.715036 | Va cost: 0.555539 | Va accuracy: 0.713480\n",
      "Epoch 48 | Tr cost: 0.553585 | Tr accuracy 0.715689 | Va cost: 0.554612 | Va accuracy: 0.714425\n",
      "Epoch 49 | Tr cost: 0.552670 | Tr accuracy 0.716401 | Va cost: 0.553659 | Va accuracy: 0.715540\n",
      "Epoch 50 | Tr cost: 0.551710 | Tr accuracy 0.717170 | Va cost: 0.552666 | Va accuracy: 0.716180\n",
      "Epoch 51 | Tr cost: 0.550749 | Tr accuracy 0.717721 | Va cost: 0.551684 | Va accuracy: 0.716750\n",
      "Epoch 52 | Tr cost: 0.549797 | Tr accuracy 0.718322 | Va cost: 0.550773 | Va accuracy: 0.717265\n",
      "Epoch 53 | Tr cost: 0.548878 | Tr accuracy 0.718916 | Va cost: 0.549961 | Va accuracy: 0.717985\n",
      "Epoch 54 | Tr cost: 0.548004 | Tr accuracy 0.719441 | Va cost: 0.549215 | Va accuracy: 0.718330\n",
      "Epoch 55 | Tr cost: 0.547198 | Tr accuracy 0.720066 | Va cost: 0.548479 | Va accuracy: 0.718750\n",
      "Epoch 56 | Tr cost: 0.546443 | Tr accuracy 0.720572 | Va cost: 0.547731 | Va accuracy: 0.719070\n",
      "Epoch 57 | Tr cost: 0.545718 | Tr accuracy 0.721136 | Va cost: 0.546981 | Va accuracy: 0.719505\n",
      "Epoch 58 | Tr cost: 0.545016 | Tr accuracy 0.721515 | Va cost: 0.546260 | Va accuracy: 0.719915\n",
      "Epoch 59 | Tr cost: 0.544343 | Tr accuracy 0.721849 | Va cost: 0.545606 | Va accuracy: 0.720070\n",
      "Epoch 60 | Tr cost: 0.543708 | Tr accuracy 0.722170 | Va cost: 0.545028 | Va accuracy: 0.720375\n",
      "Epoch 61 | Tr cost: 0.543112 | Tr accuracy 0.722485 | Va cost: 0.544496 | Va accuracy: 0.720955\n",
      "Epoch 62 | Tr cost: 0.542548 | Tr accuracy 0.722747 | Va cost: 0.543977 | Va accuracy: 0.721520\n",
      "Epoch 63 | Tr cost: 0.542013 | Tr accuracy 0.723080 | Va cost: 0.543479 | Va accuracy: 0.721900\n",
      "Epoch 64 | Tr cost: 0.541517 | Tr accuracy 0.723243 | Va cost: 0.543018 | Va accuracy: 0.722060\n",
      "Epoch 65 | Tr cost: 0.541065 | Tr accuracy 0.723522 | Va cost: 0.542589 | Va accuracy: 0.722250\n",
      "Epoch 66 | Tr cost: 0.540656 | Tr accuracy 0.723758 | Va cost: 0.542195 | Va accuracy: 0.722490\n",
      "Epoch 67 | Tr cost: 0.540284 | Tr accuracy 0.724054 | Va cost: 0.541843 | Va accuracy: 0.722855\n",
      "Epoch 68 | Tr cost: 0.539945 | Tr accuracy 0.724391 | Va cost: 0.541524 | Va accuracy: 0.723175\n",
      "Epoch 69 | Tr cost: 0.539631 | Tr accuracy 0.724537 | Va cost: 0.541209 | Va accuracy: 0.723115\n",
      "Epoch 70 | Tr cost: 0.539328 | Tr accuracy 0.724755 | Va cost: 0.540873 | Va accuracy: 0.723540\n",
      "Epoch 71 | Tr cost: 0.539020 | Tr accuracy 0.724935 | Va cost: 0.540508 | Va accuracy: 0.723655\n",
      "Epoch 72 | Tr cost: 0.538699 | Tr accuracy 0.725158 | Va cost: 0.540139 | Va accuracy: 0.723845\n",
      "Epoch 73 | Tr cost: 0.538376 | Tr accuracy 0.725319 | Va cost: 0.539794 | Va accuracy: 0.723915\n",
      "Epoch 74 | Tr cost: 0.538064 | Tr accuracy 0.725437 | Va cost: 0.539478 | Va accuracy: 0.724250\n",
      "Epoch 75 | Tr cost: 0.537767 | Tr accuracy 0.725621 | Va cost: 0.539186 | Va accuracy: 0.724585\n",
      "Epoch 76 | Tr cost: 0.537483 | Tr accuracy 0.725910 | Va cost: 0.538915 | Va accuracy: 0.724750\n",
      "Epoch 77 | Tr cost: 0.537208 | Tr accuracy 0.726056 | Va cost: 0.538664 | Va accuracy: 0.724870\n",
      "Epoch 78 | Tr cost: 0.536943 | Tr accuracy 0.726229 | Va cost: 0.538426 | Va accuracy: 0.724915\n",
      "Epoch 79 | Tr cost: 0.536682 | Tr accuracy 0.726511 | Va cost: 0.538187 | Va accuracy: 0.725325\n",
      "Epoch 80 | Tr cost: 0.536417 | Tr accuracy 0.726611 | Va cost: 0.537942 | Va accuracy: 0.725655\n",
      "Epoch 81 | Tr cost: 0.536149 | Tr accuracy 0.726904 | Va cost: 0.537698 | Va accuracy: 0.725805\n",
      "Epoch 82 | Tr cost: 0.535883 | Tr accuracy 0.727069 | Va cost: 0.537461 | Va accuracy: 0.725900\n",
      "Epoch 83 | Tr cost: 0.535625 | Tr accuracy 0.727228 | Va cost: 0.537240 | Va accuracy: 0.725940\n",
      "Epoch 84 | Tr cost: 0.535381 | Tr accuracy 0.727360 | Va cost: 0.537035 | Va accuracy: 0.725945\n",
      "Epoch 85 | Tr cost: 0.535151 | Tr accuracy 0.727428 | Va cost: 0.536846 | Va accuracy: 0.726080\n",
      "Epoch 86 | Tr cost: 0.534935 | Tr accuracy 0.727615 | Va cost: 0.536670 | Va accuracy: 0.726320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87 | Tr cost: 0.534730 | Tr accuracy 0.727629 | Va cost: 0.536502 | Va accuracy: 0.726295\n",
      "Epoch 88 | Tr cost: 0.534529 | Tr accuracy 0.727759 | Va cost: 0.536334 | Va accuracy: 0.726240\n",
      "Epoch 89 | Tr cost: 0.534329 | Tr accuracy 0.727794 | Va cost: 0.536158 | Va accuracy: 0.726505\n",
      "Epoch 90 | Tr cost: 0.534126 | Tr accuracy 0.727856 | Va cost: 0.535972 | Va accuracy: 0.726620\n",
      "Epoch 91 | Tr cost: 0.533920 | Tr accuracy 0.728017 | Va cost: 0.535783 | Va accuracy: 0.726755\n",
      "Epoch 92 | Tr cost: 0.533715 | Tr accuracy 0.728102 | Va cost: 0.535597 | Va accuracy: 0.726750\n",
      "Epoch 93 | Tr cost: 0.533514 | Tr accuracy 0.728228 | Va cost: 0.535421 | Va accuracy: 0.726815\n",
      "Epoch 94 | Tr cost: 0.533322 | Tr accuracy 0.728374 | Va cost: 0.535253 | Va accuracy: 0.727040\n",
      "Epoch 95 | Tr cost: 0.533140 | Tr accuracy 0.728481 | Va cost: 0.535092 | Va accuracy: 0.727240\n",
      "Epoch 96 | Tr cost: 0.532969 | Tr accuracy 0.728619 | Va cost: 0.534932 | Va accuracy: 0.727435\n",
      "Epoch 97 | Tr cost: 0.532807 | Tr accuracy 0.728781 | Va cost: 0.534768 | Va accuracy: 0.727535\n",
      "Epoch 98 | Tr cost: 0.532650 | Tr accuracy 0.728875 | Va cost: 0.534597 | Va accuracy: 0.727570\n",
      "Epoch 99 | Tr cost: 0.532499 | Tr accuracy 0.729130 | Va cost: 0.534419 | Va accuracy: 0.727795\n",
      "Epoch 100 | Tr cost: 0.532352 | Tr accuracy 0.729221 | Va cost: 0.534235 | Va accuracy: 0.727820\n",
      "Epoch 101 | Tr cost: 0.532206 | Tr accuracy 0.729260 | Va cost: 0.534054 | Va accuracy: 0.727915\n",
      "Epoch 102 | Tr cost: 0.532064 | Tr accuracy 0.729355 | Va cost: 0.533880 | Va accuracy: 0.728130\n",
      "Epoch 103 | Tr cost: 0.531927 | Tr accuracy 0.729434 | Va cost: 0.533714 | Va accuracy: 0.728195\n",
      "Epoch 104 | Tr cost: 0.531799 | Tr accuracy 0.729492 | Va cost: 0.533558 | Va accuracy: 0.728465\n",
      "Epoch 105 | Tr cost: 0.531680 | Tr accuracy 0.729588 | Va cost: 0.533411 | Va accuracy: 0.728450\n",
      "Epoch 106 | Tr cost: 0.531570 | Tr accuracy 0.729664 | Va cost: 0.533271 | Va accuracy: 0.728465\n",
      "Epoch 107 | Tr cost: 0.531465 | Tr accuracy 0.729746 | Va cost: 0.533137 | Va accuracy: 0.728465\n",
      "Epoch 108 | Tr cost: 0.531365 | Tr accuracy 0.729873 | Va cost: 0.533007 | Va accuracy: 0.728680\n",
      "Epoch 109 | Tr cost: 0.531267 | Tr accuracy 0.729962 | Va cost: 0.532882 | Va accuracy: 0.728780\n",
      "Epoch 110 | Tr cost: 0.531171 | Tr accuracy 0.729934 | Va cost: 0.532763 | Va accuracy: 0.728875\n",
      "Epoch 111 | Tr cost: 0.531078 | Tr accuracy 0.729984 | Va cost: 0.532654 | Va accuracy: 0.728975\n",
      "Epoch 112 | Tr cost: 0.530988 | Tr accuracy 0.730023 | Va cost: 0.532554 | Va accuracy: 0.728900\n",
      "Epoch 113 | Tr cost: 0.530903 | Tr accuracy 0.730009 | Va cost: 0.532464 | Va accuracy: 0.729015\n",
      "Epoch 114 | Tr cost: 0.530824 | Tr accuracy 0.730014 | Va cost: 0.532383 | Va accuracy: 0.729105\n",
      "Epoch 115 | Tr cost: 0.530752 | Tr accuracy 0.730124 | Va cost: 0.532307 | Va accuracy: 0.729300\n",
      "Epoch 116 | Tr cost: 0.530684 | Tr accuracy 0.730143 | Va cost: 0.532233 | Va accuracy: 0.729285\n",
      "Epoch 117 | Tr cost: 0.530619 | Tr accuracy 0.730137 | Va cost: 0.532157 | Va accuracy: 0.729370\n",
      "Epoch 118 | Tr cost: 0.530555 | Tr accuracy 0.730175 | Va cost: 0.532077 | Va accuracy: 0.729400\n",
      "Epoch 119 | Tr cost: 0.530491 | Tr accuracy 0.730232 | Va cost: 0.531994 | Va accuracy: 0.729535\n",
      "Epoch 120 | Tr cost: 0.530426 | Tr accuracy 0.730266 | Va cost: 0.531908 | Va accuracy: 0.729660\n",
      "Epoch 121 | Tr cost: 0.530362 | Tr accuracy 0.730307 | Va cost: 0.531824 | Va accuracy: 0.729680\n",
      "Epoch 122 | Tr cost: 0.530299 | Tr accuracy 0.730336 | Va cost: 0.531743 | Va accuracy: 0.729825\n",
      "Epoch 123 | Tr cost: 0.530240 | Tr accuracy 0.730376 | Va cost: 0.531667 | Va accuracy: 0.729870\n",
      "Epoch 124 | Tr cost: 0.530183 | Tr accuracy 0.730464 | Va cost: 0.531596 | Va accuracy: 0.729685\n",
      "Epoch 125 | Tr cost: 0.530127 | Tr accuracy 0.730529 | Va cost: 0.531527 | Va accuracy: 0.729610\n",
      "Epoch 126 | Tr cost: 0.530070 | Tr accuracy 0.730503 | Va cost: 0.531460 | Va accuracy: 0.729560\n",
      "Epoch 127 | Tr cost: 0.530011 | Tr accuracy 0.730612 | Va cost: 0.531392 | Va accuracy: 0.729660\n",
      "Epoch 128 | Tr cost: 0.529947 | Tr accuracy 0.730684 | Va cost: 0.531324 | Va accuracy: 0.729715\n",
      "Epoch 129 | Tr cost: 0.529879 | Tr accuracy 0.730771 | Va cost: 0.531258 | Va accuracy: 0.729705\n",
      "Epoch 130 | Tr cost: 0.529809 | Tr accuracy 0.730760 | Va cost: 0.531196 | Va accuracy: 0.729685\n",
      "Epoch 131 | Tr cost: 0.529740 | Tr accuracy 0.730810 | Va cost: 0.531139 | Va accuracy: 0.729725\n",
      "Epoch 132 | Tr cost: 0.529676 | Tr accuracy 0.730806 | Va cost: 0.531089 | Va accuracy: 0.729800\n",
      "Epoch 133 | Tr cost: 0.529618 | Tr accuracy 0.730900 | Va cost: 0.531045 | Va accuracy: 0.729790\n",
      "Epoch 134 | Tr cost: 0.529566 | Tr accuracy 0.730979 | Va cost: 0.531006 | Va accuracy: 0.729865\n",
      "Epoch 135 | Tr cost: 0.529519 | Tr accuracy 0.731019 | Va cost: 0.530968 | Va accuracy: 0.729990\n",
      "Epoch 136 | Tr cost: 0.529475 | Tr accuracy 0.731040 | Va cost: 0.530927 | Va accuracy: 0.730075\n",
      "Epoch 137 | Tr cost: 0.529430 | Tr accuracy 0.731088 | Va cost: 0.530882 | Va accuracy: 0.730075\n",
      "Epoch 138 | Tr cost: 0.529382 | Tr accuracy 0.731056 | Va cost: 0.530832 | Va accuracy: 0.730140\n",
      "Epoch 139 | Tr cost: 0.529331 | Tr accuracy 0.731110 | Va cost: 0.530780 | Va accuracy: 0.730130\n",
      "Epoch 140 | Tr cost: 0.529277 | Tr accuracy 0.731171 | Va cost: 0.530729 | Va accuracy: 0.730190\n",
      "Epoch 141 | Tr cost: 0.529222 | Tr accuracy 0.731141 | Va cost: 0.530683 | Va accuracy: 0.730415\n",
      "Epoch 142 | Tr cost: 0.529168 | Tr accuracy 0.731155 | Va cost: 0.530642 | Va accuracy: 0.730445\n",
      "Epoch 143 | Tr cost: 0.529117 | Tr accuracy 0.731189 | Va cost: 0.530606 | Va accuracy: 0.730500\n",
      "Epoch 144 | Tr cost: 0.529070 | Tr accuracy 0.731213 | Va cost: 0.530571 | Va accuracy: 0.730560\n",
      "Epoch 145 | Tr cost: 0.529025 | Tr accuracy 0.731234 | Va cost: 0.530534 | Va accuracy: 0.730640\n",
      "Epoch 146 | Tr cost: 0.528980 | Tr accuracy 0.731225 | Va cost: 0.530494 | Va accuracy: 0.730675\n",
      "Epoch 147 | Tr cost: 0.528935 | Tr accuracy 0.731229 | Va cost: 0.530448 | Va accuracy: 0.730695\n",
      "Epoch 148 | Tr cost: 0.528888 | Tr accuracy 0.731285 | Va cost: 0.530398 | Va accuracy: 0.730675\n",
      "Epoch 149 | Tr cost: 0.528838 | Tr accuracy 0.731286 | Va cost: 0.530344 | Va accuracy: 0.730790\n",
      "Epoch 150 | Tr cost: 0.528786 | Tr accuracy 0.731236 | Va cost: 0.530291 | Va accuracy: 0.730865\n",
      "Epoch 151 | Tr cost: 0.528734 | Tr accuracy 0.731241 | Va cost: 0.530239 | Va accuracy: 0.730780\n",
      "Epoch 152 | Tr cost: 0.528682 | Tr accuracy 0.731254 | Va cost: 0.530190 | Va accuracy: 0.730830\n",
      "Epoch 153 | Tr cost: 0.528632 | Tr accuracy 0.731331 | Va cost: 0.530145 | Va accuracy: 0.730920\n",
      "Epoch 154 | Tr cost: 0.528583 | Tr accuracy 0.731400 | Va cost: 0.530101 | Va accuracy: 0.730965\n",
      "Epoch 155 | Tr cost: 0.528536 | Tr accuracy 0.731455 | Va cost: 0.530058 | Va accuracy: 0.730970\n",
      "Epoch 156 | Tr cost: 0.528491 | Tr accuracy 0.731440 | Va cost: 0.530015 | Va accuracy: 0.730965\n",
      "Epoch 157 | Tr cost: 0.528446 | Tr accuracy 0.731418 | Va cost: 0.529970 | Va accuracy: 0.730930\n",
      "Epoch 158 | Tr cost: 0.528400 | Tr accuracy 0.731406 | Va cost: 0.529925 | Va accuracy: 0.731005\n",
      "Epoch 159 | Tr cost: 0.528355 | Tr accuracy 0.731407 | Va cost: 0.529878 | Va accuracy: 0.731050\n",
      "Epoch 160 | Tr cost: 0.528308 | Tr accuracy 0.731468 | Va cost: 0.529831 | Va accuracy: 0.731120\n",
      "Epoch 161 | Tr cost: 0.528262 | Tr accuracy 0.731511 | Va cost: 0.529784 | Va accuracy: 0.731140\n",
      "Epoch 162 | Tr cost: 0.528216 | Tr accuracy 0.731554 | Va cost: 0.529739 | Va accuracy: 0.731140\n",
      "Epoch 163 | Tr cost: 0.528171 | Tr accuracy 0.731594 | Va cost: 0.529695 | Va accuracy: 0.731170\n",
      "Epoch 164 | Tr cost: 0.528127 | Tr accuracy 0.731549 | Va cost: 0.529653 | Va accuracy: 0.731175\n",
      "Epoch 165 | Tr cost: 0.528085 | Tr accuracy 0.731577 | Va cost: 0.529612 | Va accuracy: 0.731290\n",
      "Epoch 166 | Tr cost: 0.528043 | Tr accuracy 0.731571 | Va cost: 0.529571 | Va accuracy: 0.731265\n",
      "Epoch 167 | Tr cost: 0.528003 | Tr accuracy 0.731600 | Va cost: 0.529530 | Va accuracy: 0.731245\n",
      "Epoch 168 | Tr cost: 0.527963 | Tr accuracy 0.731653 | Va cost: 0.529488 | Va accuracy: 0.731235\n",
      "Epoch 169 | Tr cost: 0.527923 | Tr accuracy 0.731706 | Va cost: 0.529445 | Va accuracy: 0.731285\n",
      "Epoch 170 | Tr cost: 0.527882 | Tr accuracy 0.731705 | Va cost: 0.529400 | Va accuracy: 0.731225\n",
      "Epoch 171 | Tr cost: 0.527842 | Tr accuracy 0.731704 | Va cost: 0.529355 | Va accuracy: 0.731270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172 | Tr cost: 0.527801 | Tr accuracy 0.731734 | Va cost: 0.529309 | Va accuracy: 0.731295\n",
      "Epoch 173 | Tr cost: 0.527760 | Tr accuracy 0.731742 | Va cost: 0.529265 | Va accuracy: 0.731305\n",
      "Epoch 174 | Tr cost: 0.527720 | Tr accuracy 0.731733 | Va cost: 0.529221 | Va accuracy: 0.731295\n",
      "Epoch 175 | Tr cost: 0.527681 | Tr accuracy 0.731753 | Va cost: 0.529180 | Va accuracy: 0.731345\n",
      "Epoch 176 | Tr cost: 0.527642 | Tr accuracy 0.731719 | Va cost: 0.529140 | Va accuracy: 0.731330\n",
      "Epoch 177 | Tr cost: 0.527605 | Tr accuracy 0.731721 | Va cost: 0.529101 | Va accuracy: 0.731260\n",
      "Epoch 178 | Tr cost: 0.527569 | Tr accuracy 0.731741 | Va cost: 0.529063 | Va accuracy: 0.731285\n",
      "Epoch 179 | Tr cost: 0.527534 | Tr accuracy 0.731768 | Va cost: 0.529026 | Va accuracy: 0.731275\n",
      "Epoch 180 | Tr cost: 0.527500 | Tr accuracy 0.731766 | Va cost: 0.528989 | Va accuracy: 0.731340\n",
      "Epoch 181 | Tr cost: 0.527466 | Tr accuracy 0.731836 | Va cost: 0.528952 | Va accuracy: 0.731385\n",
      "Epoch 182 | Tr cost: 0.527432 | Tr accuracy 0.731849 | Va cost: 0.528914 | Va accuracy: 0.731420\n",
      "Epoch 183 | Tr cost: 0.527398 | Tr accuracy 0.731874 | Va cost: 0.528877 | Va accuracy: 0.731470\n",
      "Epoch 184 | Tr cost: 0.527364 | Tr accuracy 0.731896 | Va cost: 0.528840 | Va accuracy: 0.731630\n",
      "Epoch 185 | Tr cost: 0.527331 | Tr accuracy 0.731923 | Va cost: 0.528804 | Va accuracy: 0.731685\n",
      "Epoch 186 | Tr cost: 0.527298 | Tr accuracy 0.731971 | Va cost: 0.528768 | Va accuracy: 0.731690\n",
      "Epoch 187 | Tr cost: 0.527266 | Tr accuracy 0.732004 | Va cost: 0.528734 | Va accuracy: 0.731745\n",
      "Epoch 188 | Tr cost: 0.527234 | Tr accuracy 0.732039 | Va cost: 0.528701 | Va accuracy: 0.731765\n",
      "Epoch 189 | Tr cost: 0.527202 | Tr accuracy 0.732064 | Va cost: 0.528669 | Va accuracy: 0.731815\n",
      "Epoch 190 | Tr cost: 0.527171 | Tr accuracy 0.732079 | Va cost: 0.528638 | Va accuracy: 0.731955\n",
      "Epoch 191 | Tr cost: 0.527141 | Tr accuracy 0.732106 | Va cost: 0.528608 | Va accuracy: 0.731935\n",
      "Epoch 192 | Tr cost: 0.527111 | Tr accuracy 0.732121 | Va cost: 0.528578 | Va accuracy: 0.731880\n",
      "Epoch 193 | Tr cost: 0.527081 | Tr accuracy 0.732136 | Va cost: 0.528548 | Va accuracy: 0.731875\n",
      "Epoch 194 | Tr cost: 0.527051 | Tr accuracy 0.732160 | Va cost: 0.528517 | Va accuracy: 0.731885\n",
      "Epoch 195 | Tr cost: 0.527022 | Tr accuracy 0.732201 | Va cost: 0.528487 | Va accuracy: 0.731905\n",
      "Epoch 196 | Tr cost: 0.526992 | Tr accuracy 0.732260 | Va cost: 0.528456 | Va accuracy: 0.731855\n",
      "Epoch 197 | Tr cost: 0.526963 | Tr accuracy 0.732259 | Va cost: 0.528426 | Va accuracy: 0.731925\n",
      "Epoch 198 | Tr cost: 0.526933 | Tr accuracy 0.732256 | Va cost: 0.528395 | Va accuracy: 0.731920\n",
      "Epoch 199 | Tr cost: 0.526904 | Tr accuracy 0.732290 | Va cost: 0.528364 | Va accuracy: 0.732055\n",
      "Epoch 200 | Tr cost: 0.526874 | Tr accuracy 0.732337 | Va cost: 0.528334 | Va accuracy: 0.732070\n",
      "Optimization Finished!\n",
      "Calculating predictions of the model on the test set\n",
      "Inference done. Now saving the predictions in a list.\n"
     ]
    }
   ],
   "source": [
    "training_costs, validation_costs, training_accs, validation_accs, test_predictions = train_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "typechecks_v",
     "locked": true,
     "points": "5",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## making sure your train_nn() function returns variables as expected\n",
    "assert type(training_accs)==list,\"incorrect return type\"\n",
    "assert type(validation_accs)==list, \"incorrect return type\"\n",
    "assert type(training_costs)==list, \"incorrect return type\"\n",
    "assert type(validation_costs)==list,\"incorrect return type\"\n",
    "assert type(test_predictions[0])==type(training_accs[0])==type(validation_accs[0])==type(training_costs[0])==type(validation_costs[0])==np.float64, \"incorrect return type\"\n",
    "assert len(training_accs)==len(validation_accs)==len(training_costs)==len(validation_costs)==training_epochs,\"incorrect returned lengths\"\n",
    "assert len(test_predictions)==len(test_features),\"incorrect returned lengths\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4ed33d0856316a08",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Analyzing cost and accuracy trends vs number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-450747d7f0431e00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_cost_vs_epochs(training_costs, validation_costs):\n",
    "    plt.title(\"cost vs epochs\")\n",
    "    plt.plot(training_costs)\n",
    "    plt.plot(validation_costs)\n",
    "    plt.legend([\"training\",\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-78d0e31f3ed4005e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_acc_vs_epochs(training_acc, validation_acc):\n",
    "    plt.title(\"accuracy vs epochs\")\n",
    "    plt.plot(training_acc)\n",
    "    plt.plot(validation_acc)\n",
    "    plt.legend([\"training\",\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5f2c5d57928f7a69",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def plots(training_costs, validation_costs, training_accs, validation_accs):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plot_cost_vs_epochs(training_costs, validation_costs)\n",
    "    plt.subplot(1,2,2)\n",
    "    plot_acc_vs_epochs(training_accs, validation_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plots(training_costs, validation_costs, training_accs, validation_accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e48102290dfc483",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Scoring Your Test Predictions\n",
    "\n",
    "The following code saves your test predictions locally in the same directory.\n",
    "\n",
    "It is required in order to give you a grade based on your network's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9130fc95b3adda93",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def save_test_predictions(test_predictions):\n",
    "     with open(cfg['output_predictions_pickle_path'], 'wb') as f:\n",
    "        pickle.dump(test_predictions, f)\n",
    "save_test_predictions(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "threshold1_h",
     "locked": true,
     "points": "5",
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.732075\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Hidden tests here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "threshold2_h",
     "locked": true,
     "points": "10",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Hidden tests here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "threshold3_h",
     "locked": true,
     "points": "15",
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#Hidden tests here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": true,
     "grade_id": "threshold4_h",
     "locked": true,
     "points": "20",
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Hidden tests here\n",
    "#\n",
    "# AUTOGRADER TEST - DO NOT REMOVE\n",
    "#\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": [],
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": [],
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
